[
  {
    "objectID": "posts/hp-book/index.html#some-links-to-fan-made-stories",
    "href": "posts/hp-book/index.html#some-links-to-fan-made-stories",
    "title": "A better Harry Potter Book",
    "section": "Some links to fan made stories",
    "text": "Some links to fan made stories\n\nThe Time In Between: This is a look at the developing relationship between Hermione and Ron set during the Deathly Hallows.\nHermione Granger Series: This fan rewrote the first four books from Hermione’s point of view only. The links to each storie:\n\nHermione Granger and the Philosopher’s Stone\nHermione Granger and the Chamber of Secrets\nHermione Granger and the Prisoner of Azkaban\nHermione Granger and the Goblet of Fire"
  },
  {
    "objectID": "spanish.html",
    "href": "spanish.html",
    "title": "Blog en Español",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\nFor code, please refer to the license of each respective repository."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A better Harry Potter Book\n\n\nWhat would happen if the book had more points of view? \n\n\n\nFantasy\n\n\nbooks\n\n\nharry-potter\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nManuel Andersen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Data Engineering/week-01/07-gcp.html",
    "href": "notes/Data Engineering/week-01/07-gcp.html",
    "title": "GCP",
    "section": "",
    "text": "GCP: The Google Cloud console provides a web-based, graphical user interface that you can use to manage your Google Cloud projects and resources."
  },
  {
    "objectID": "notes/Data Engineering/week-01/07-gcp.html#terminology",
    "href": "notes/Data Engineering/week-01/07-gcp.html#terminology",
    "title": "GCP",
    "section": "",
    "text": "GCP: The Google Cloud console provides a web-based, graphical user interface that you can use to manage your Google Cloud projects and resources."
  },
  {
    "objectID": "notes/Data Engineering/week-01/07-gcp.html#useful-links",
    "href": "notes/Data Engineering/week-01/07-gcp.html#useful-links",
    "title": "GCP",
    "section": "Useful links",
    "text": "Useful links\n\nGoogle Cloud"
  },
  {
    "objectID": "notes/Data Engineering/week-01/07-gcp.html#what-is-gcp",
    "href": "notes/Data Engineering/week-01/07-gcp.html#what-is-gcp",
    "title": "GCP",
    "section": "What is GCP?",
    "text": "What is GCP?\nGCP stands for Google Cloud Plataform, and it’s a:\n\nCloud computing services offered by google.\nIncludes a range of hosted services for compute, storage and application development that runs on Google hardware.\nSame hardware on which google runs its service."
  },
  {
    "objectID": "notes/Data Engineering/week-01/07-gcp.html#what-are-we-doing",
    "href": "notes/Data Engineering/week-01/07-gcp.html#what-are-we-doing",
    "title": "GCP",
    "section": "What are we doing?",
    "text": "What are we doing?\nIn the following chapters we will setup a Data Lake on Google Cloud Storage and a Data Warehouse in BigQuery. We will explore these concepts in future lessons but a Data Lake is where we would usually store data and a Data Warehouse provides a more structured way to access this data.\nWe need to setup access first by assigning the Storage Admin, Storage Object Admin, BigQuery Admin and Viewer IAM roles to the Service Account, and then enable the iam and iamcredentials APIs for our project.\n\nAssign the following IAM Roles to the Service Account: Storage Admin, Storage Object Admin, BigQuery Admin and Viewer.\n\nOn the GCP Project dashboard, go to IAM & Admin &gt; IAM\nSelect the previously created Service Account and edit the permissions by clicking on the pencil shaped icon on the left.\nAdd the following roles and click on Save afterwards:\n\nStorage Admin: for creating and managing buckets.\nStorage Object Admin: for creating and managing objects within the buckets.\nBigQuery Admin: for managing BigQuery resources and data.\nViewer should already be present as a role.\n\n\nEnable APIs for the project (these are needed so that Terraform can interact with GCP):\n\nhttps://console.cloud.google.com/apis/library/iam.googleapis.com\nhttps://console.cloud.google.com/apis/library/iamcredentials.googleapis.com\n\nMake sure that the GOOGLE_APPLICATION_CREDENTIALS environment variable is set."
  },
  {
    "objectID": "notes/Data Engineering/week-01/05-docker-compose.html",
    "href": "notes/Data Engineering/week-01/05-docker-compose.html",
    "title": "Docker-Compose",
    "section": "",
    "text": "docker-compose: is a convenient way to run multiple related services with just one config file."
  },
  {
    "objectID": "notes/Data Engineering/week-01/05-docker-compose.html#terminology",
    "href": "notes/Data Engineering/week-01/05-docker-compose.html#terminology",
    "title": "Docker-Compose",
    "section": "",
    "text": "docker-compose: is a convenient way to run multiple related services with just one config file."
  },
  {
    "objectID": "notes/Data Engineering/week-01/05-docker-compose.html#useful-links",
    "href": "notes/Data Engineering/week-01/05-docker-compose.html#useful-links",
    "title": "Docker-Compose",
    "section": "Useful links",
    "text": "Useful links\n\ndocker compose official page."
  },
  {
    "objectID": "notes/Data Engineering/week-01/05-docker-compose.html#what-is-docker-compose",
    "href": "notes/Data Engineering/week-01/05-docker-compose.html#what-is-docker-compose",
    "title": "Docker-Compose",
    "section": "What is Docker Compose?",
    "text": "What is Docker Compose?\ndocker-compose allows us to launch multiple containers using a single configuration file, so that we don’t have to run multiple complex docker run commands separately.\nFirst of all, we need to create a file called docker-compose.yaml:\nservices:\n  pgdatabase:\n    image: postgres:13\n    environment:\n      - POSTGRES_USER=root\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=ny_taxi\n    volumes:\n      - \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\n    ports:\n      - \"5432:5432\"\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    volumes:\n      - \"./data_pgadmin:/var/lib/pgadmin\"\n    ports:\n      - \"8080:80\"\nhere:\n\nservices will be all the services we want in the compose, in our case we want the pgdatabase and pgadmin.\nimage is the image we want for every service.\nenvironment are all the environment variables each services needs to run.\nvolumes is for doing volume mapping in the form hostPath:containerPath:mode\nports are for accesing the ports, in the form hostPort:containerPort\n\nNow, before running the docker-compose make sure you stop every container by cheking with docker ps. Now for running the compose we do:\ndocker-compose up\na few notes:\n\nWe don’t have to specify a network because docker-compose takes care of it: every single container (or “service”, as the file states) will run withing the same network and will be able to find each other according to their names (pgdatabase and pgadmin in this example).\nWe’ve added a volume for pgAdmin to save its settings, so that you don’t have to keep re-creating the connection to Postgres every time ypu rerun the container. Make sure you create a data_pgadmin directory in your work folder where you run docker-compose from.\nAll other details from the docker run commands (environment variables, volumes and ports) are mentioned accordingly in the file following YAML syntax.\n\nFor shutting down, first you type Ctrl+C and then\ndocker-compose down"
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires.\n\n\n\n\n\npgAdmin official page.\n\n\n\n\nWe can start looking at our database in the postgres CLI by doing\nSELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount) \nFROM yellow_taxi_data;\nwich will give us something like this:\n\nBut interacting this way with the database is not great. For a more convenient way we will use pgAdmin which is a web based GUI tool.\nInstead of installing it, we will use dokcer to install a container with pgAdmin, tu run the container we will use:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\nremembre, we use -e to declare environment variables, -p to do port mapping, so here:\n\nPGADMIN_DEFAULT_EMAIL is the default username we will use to login in pgAdmin.\nPGADMIN_DEFAULT_PASSWORD is the default password.\n8080:80 8080 is our local machine port and we will send request to the port 80 on the container.\ndpage/pgadmin4 is the pgAdmin image.\n\nwe the run this, and in our browser we go to localhost:8080 and we will see something like this:\n\nhere we use the credentials we give as environment variables. And we will get something like:\n\nWe now need to create a server, for wich we do:\n\nWe then put a name to our server:\n\nand then go to the Connection tab and add the Host name to localhost and use root as the username and password like we did with our engine to load the database, BUT we will encounter a problem:\n\nWhy is that? It’s because we are running the pgAdmin inside a container, different to the container where we are running postgres. So, when we put localhost in the pgAdmin Host name, pgAdmin goes and look for localhost and it will be the container in wich we are running pgAdmin. We need to conect them, to do this we do Docker Network.\nFirst, let’s stop both containers. Then we need to create the docker network by doing:\ndocker  network create pg-network\nhere:\n\npg-network is the name of the network we want to create.\n\nNow, we need to initiate the postgres container in this way:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    --network=pg-network \\\n    --name pg-database \\\n    postgres:13  \nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-database is the name wich pgAdmin will use to discover the database.\n\nAnd now we need to initiate the pgAdmin container with:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  --name pg-admin \\\n  dpage/pgadmin4\nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-admin is the name of this container.\n\nNotes:\n\nIMPORTANT: these are example values for testing and should never be used on production. Change them accordingly when needed.\npgAdmin is a web app and its default port is 80; we map it to 8080 in our localhost to avoid any possible conflicts.\nJust like with the Postgres container, we specify a network and a name. However, the name in this example isn’t really necessary because there won’t be any containers trying to access this particular container.\n\nWe now run this and go to our webrowser and go to localhost:8080, and now we create a server, for wich we do:\n\nWe then put a name to our server:\n\nand then go to the Connection tab and add the Host name to pg-database and use root as the username and password like we did with our engine to load the database like this:\n\nwe save, and we will have our server! Now, lets say we want to look at the first 100 rows we do:\n(Right Side Menu) &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; public &gt; Tables &gt; yellow_taxi_data &gt; (left_click) &gt; View/Edit Data &gt; First 100 Rows, and we will have something like this:"
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#terminology",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#terminology",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires."
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#useful-links",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#useful-links",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin official page."
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#what-is-pgadmin",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#what-is-pgadmin",
    "title": "pgAdmin",
    "section": "",
    "text": "We can start looking at our database in the postgres CLI by doing\nSELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount) \nFROM yellow_taxi_data;\nwich will give us something like this:\n\nBut interacting this way with the database is not great. For a more convenient way we will use pgAdmin which is a web based GUI tool.\nInstead of installing it, we will use dokcer to install a container with pgAdmin, tu run the container we will use:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\nremembre, we use -e to declare environment variables, -p to do port mapping, so here:\n\nPGADMIN_DEFAULT_EMAIL is the default username we will use to login in pgAdmin.\nPGADMIN_DEFAULT_PASSWORD is the default password.\n8080:80 8080 is our local machine port and we will send request to the port 80 on the container.\ndpage/pgadmin4 is the pgAdmin image.\n\nwe the run this, and in our browser we go to localhost:8080 and we will see something like this:\n\nhere we use the credentials we give as environment variables. And we will get something like:\n\nWe now need to create a server, for wich we do:\n\nWe then put a name to our server:\n\nand then go to the Connection tab and add the Host name to localhost and use root as the username and password like we did with our engine to load the database, BUT we will encounter a problem:\n\nWhy is that? It’s because we are running the pgAdmin inside a container, different to the container where we are running postgres. So, when we put localhost in the pgAdmin Host name, pgAdmin goes and look for localhost and it will be the container in wich we are running pgAdmin. We need to conect them, to do this we do Docker Network.\nFirst, let’s stop both containers. Then we need to create the docker network by doing:\ndocker  network create pg-network\nhere:\n\npg-network is the name of the network we want to create.\n\nNow, we need to initiate the postgres container in this way:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    --network=pg-network \\\n    --name pg-database \\\n    postgres:13  \nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-database is the name wich pgAdmin will use to discover the database.\n\nAnd now we need to initiate the pgAdmin container with:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  --name pg-admin \\\n  dpage/pgadmin4\nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-admin is the name of this container.\n\nNotes:\n\nIMPORTANT: these are example values for testing and should never be used on production. Change them accordingly when needed.\npgAdmin is a web app and its default port is 80; we map it to 8080 in our localhost to avoid any possible conflicts.\nJust like with the Postgres container, we specify a network and a name. However, the name in this example isn’t really necessary because there won’t be any containers trying to access this particular container.\n\nWe now run this and go to our webrowser and go to localhost:8080, and now we create a server, for wich we do:\n\nWe then put a name to our server:\n\nand then go to the Connection tab and add the Host name to pg-database and use root as the username and password like we did with our engine to load the database like this:\n\nwe save, and we will have our server! Now, lets say we want to look at the first 100 rows we do:\n(Right Side Menu) &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; public &gt; Tables &gt; yellow_taxi_data &gt; (left_click) &gt; View/Edit Data &gt; First 100 Rows, and we will have something like this:"
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "title": "Docker",
    "section": "Terminology",
    "text": "Terminology\n\nContainer: environment that runs an applications that is not dependent on the OS. Kind of like a lightweight VM. Containers are stateless; if you need to update the components inside, create another container instead.\nImage: template to create a container. Its components are defined by a Dockerfile.\nVolume: storage area detached from the container for maintaining state.\nForeground/interactive vs background/detached: a detached container runs in the background whereas an interactive container will usually have a terminal of some sort for interacting with."
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "title": "Docker",
    "section": "Useful links",
    "text": "Useful links\n\nCheatsheet"
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "title": "Docker",
    "section": "What is Docker?",
    "text": "What is Docker?\nDocker is a containerization software that allows us to isolate software in a similar way to virtual machines but in a much leaner way.\nDocker delivers software in packages called containers. These containers are isolated from one another. If we run a data pipeline inside a container it is virtually isolated from the rest of the things running on the computer.\nA Docker image is a snapshot of a container that we can define to run our software, or in this case our data pipelines. By exporting our Docker images to Cloud providers such as Amazon Web Services or Google Cloud Platform we can run our containers there.\n\nWe can run the container we have created through the docker image where we have specified and configured the environment beyond the host computer and essentially everywhere like - Google Cloud (Kubernetes), AWS Batch etc.\nDocker Image ensures reproducibility regardless of the machine as the images are identical. We can specify OS, programming languages, packages, database type, tools etc. This solves the problem of “Works on my computer but NoT iN yOuRs.”\nSo, the advantages of working with Docker?\n\nLocal Experiments: Helps to run things locally like your database also helps with testing like integration testing.\nIntegration Tests (CI/CD).\nReproducibility: Docker makes things run everywhere regardless of what you are using.\nRunning Pipelines on the Cloud (AWS Batch, Kubernetes Jobs).\nSpark.\nServerless (AWS Lambda, Google Functions).\n\nAlso, Docker containers are stateless: any changes done inside a container will NOT be saved when the container is killed and started again. This is an advantage because it allows us to restore any container to its initial state in a reproducible manner, but you will have to store data elsewhere if you need to do so; a common way to do so is with volumes."
  },
  {
    "objectID": "notes/git.html",
    "href": "notes/git.html",
    "title": "Git",
    "section": "",
    "text": "So, im using this github repo to learn git and github."
  },
  {
    "objectID": "notes/git.html#first-step",
    "href": "notes/git.html#first-step",
    "title": "Git",
    "section": "First step",
    "text": "First step\nWe use:\ngit init \nto initiate a git repository. This means that our local folder will be tracked by git.\nThe second thing we do is go to github and create a repository. We use:\ngit remote add origin url\nwhere url is the url of the repository we just created.\nWe then create a python file called first-file.py and we just write in this a simple print:\nprint(\"This is the first file\")\nwe use:\ngit add .\nto add this file to our repository. Then we use:\ngit commit -m \"message\"\nto commit this files we added to the repository. You need to write in \"message\" a concise description of what you commit. Then we use:\ngit push origin master\nto push our commit to the github repository. This changes will be in the master branch.\n\nChecking what is going on\nAt any point you can do:\ngit status\nto now where the code you have write are in terms of git tracking.\nYou can also do:\ngit log\nto get a recor of all the changes and commits your code had over time. To exit this log just press q."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Manuel Andersen",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n\n  \n  \nI’m Manuel Andersen an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\n\n\n🔥 Things I’m passionate about: Python, Football, data, movies, fantasy books.\n⛰️ I’m from Concón, Chile, South America."
  },
  {
    "objectID": "about.html#personal",
    "href": "about.html#personal",
    "title": "Manuel Andersen",
    "section": "",
    "text": "🔥 Things I’m passionate about: Python, Football, data, movies, fantasy books.\n⛰️ I’m from Concón, Chile, South America."
  },
  {
    "objectID": "notes/scrapy.html",
    "href": "notes/scrapy.html",
    "title": "Scrapy",
    "section": "",
    "text": "This are my notes on this Scrapy course."
  },
  {
    "objectID": "notes/scrapy.html#setup-virtual-env-scrapy",
    "href": "notes/scrapy.html#setup-virtual-env-scrapy",
    "title": "Scrapy",
    "section": "Setup Virtual Env & Scrapy",
    "text": "Setup Virtual Env & Scrapy\nFirst of all let’s create a virtual environment by doing\npython -m venv venv\nand activate it with\nsource venv/bin/activate \nNow we can install Scrapy by doing\npip install scrapy"
  },
  {
    "objectID": "notes/scrapy.html#creating-a-scrapy-project",
    "href": "notes/scrapy.html#creating-a-scrapy-project",
    "title": "Scrapy",
    "section": "Creating a Scrapy Project",
    "text": "Creating a Scrapy Project\nTo create a new project we do:\nscrapy startproject bookscraper\nwhere bookscraper is the name of the project."
  },
  {
    "objectID": "notes/scrapy.html#first-scrapy-spider",
    "href": "notes/scrapy.html#first-scrapy-spider",
    "title": "Scrapy",
    "section": "First Scrapy Spider",
    "text": "First Scrapy Spider\nGo to the spiders file where we only have a __init__.py file and to create our first spider we run:\nscrapy genspider bookspider books.toscrape.com\nwhere bookspider is the name of the spider and books.toscrape.com is the URL of the website that we are gonna be scraping.\nand this create a very basic spider:\nimport scrapy\n\nclass BookspiderSpider(scrapy.Spider):\n    name = \"bookspider\"\n    allowed_domains = [\"books.toscrape.com\"]\n    start_urls = [\"https://books.toscrape.com\"]\n\n    def parse(self, response):\n        pass\nNow to finds css headers we are gonna use a more readable shell, so first run:\npip install ipython\nand go to the scrapy.cfg file and add shell = ipython and now we can run:\nscrapy shell\nand we can now do:\nfetch('https://books.toscrape.com/')\nif we want to look at all the data for each book:\nresponse.css('article.product_pod')\nand for the first book:\nresponse.css('article.product_pod').get()\nwe can put all the books in a variable to run another commands on it we can do:\nbooks = response.css('article.product_pod')\nso lets say we wanna figure how many book we have:\nlen(books)\ngives us 20.\nNow we are gonna get the name, the price and the URL of every book.\nFor the first book we coul do:\nbook = books[0]\nbook.css('h3 a::text').get()\nand would get 'A Light in the ...' as a response. For the price we could do:\nbook.css('.product_price .price_color::text').get()\nand would get '£51.77' as a response. For the URL we do:\nbook.css('h3 a').attrib['href']\nwich would give us 'catalogue/a-light-in-the-attic_1000/index.html'.\nSo, usign the shell we understand how to obtain the the name, the price and the URL of one book, so now we can go to parse method in our bookspider.py file, an loop trough all the 20 books. We do:\nimport scrapy\n\nclass BookspiderSpider(scrapy.Spider):\n    name = \"bookspider\"\n    allowed_domains = [\"books.toscrape.com\"]\n    start_urls = [\"https://books.toscrape.com\"]\n\n    def parse(self, response):\n        books = response.css('article.product_pod')\n\n        for book in books:\n            yield {\n                'name' : book.css('h3 a::text').get(),\n                'price' : book.css('.product_price .price_color::text').get(),\n                'url' : book.css('h3 a').attrib['href']\n            }\nAnd now (exit the shell) go back one directory in the terminal cd ../ and we can crawl the data with:\nscrapy crawl bookspider\nand that should gives us all the info. But that is for th first page, in the bottom of the page we see that there are 50 pages of books, so lets get back to our shell and try to get the url of the next page:\nresponse.css('li.next a ::attr(href)').get()\nshoul gives us 'catalogue/page-2.html'. Now, for implementing this in our spider we do:\nimport scrapy\n\nclass BookspiderSpider(scrapy.Spider):\n    name = \"bookspider\"\n    allowed_domains = [\"books.toscrape.com\"]\n    start_urls = [\"https://books.toscrape.com\"]\n\n    def parse(self, response):\n        books = response.css('article.product_pod')\n\n        for book in books:\n            yield {\n                'name' : book.css('h3 a::text').get(),\n                'price' : book.css('.product_price .price_color::text').get(),\n                'url' : book.css('h3 a').attrib['href']\n            }\n\n        next_page = response.css('li.next a ::attr(href)').get()\n\n        if next_page is not None:\n            next_page_url = 'https://books.toscrape.com/' + next_page\n            yield response.follow(next_page_url, callback = self.parse)\nnow exit the shell and run the spider."
  },
  {
    "objectID": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html",
    "href": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html",
    "title": "Dockerizing",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires.\npsychopg2: is the most popular PostgreSQL database adapter for the Python programming language.\n\n\n\n\n\npgAdmin official page.\n\n\n\n\nWe could do something like:\njupyter nbconvert --to=script upload-data.ipynb\nto convert the notebook, where we download the data, to a python file. But that’s to slow, so we are just gonna create a new file called ingest_data.py, and we will use the argparse library to pass some cli arguments:\n#Cleaned up version of data-loading.ipynb\nimport argparse, os, sys\nfrom time import time\nimport pandas as pd \nimport pyarrow.parquet as pq\nfrom sqlalchemy import create_engine\n\n\ndef main(params):\n    user = params.user\n    password = params.password\n    host = params.host\n    port = params.port\n    db = params.db\n    tb = params.tb\n    url = params.url\n    \n    # Get the name of the file from url\n    file_name = url.rsplit('/', 1)[-1].strip()\n    print(f'Downloading {file_name} ...')\n    # Download file from url\n    os.system(f'curl {url.strip()} -o {file_name}')\n    print('\\n')\n\n    # Create SQL engine\n    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n\n    # Read file based on csv or parquet\n    if '.csv' in file_name:\n        df = pd.read_csv(file_name, nrows=10)\n        df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\n    elif '.parquet' in file_name:\n        file = pq.ParquetFile(file_name)\n        df = next(file.iter_batches(batch_size=10)).to_pandas()\n        df_iter = file.iter_batches(batch_size=100000)\n    else: \n        print('Error. Only .csv or .parquet files allowed.')\n        sys.exit()\n\n\n    # Create the table\n    df.head(0).to_sql(name=tb, con=engine, if_exists='replace')\n\n\n    # Insert values\n    t_start = time()\n    count = 0\n    for batch in df_iter:\n        count+=1\n\n        if '.parquet' in file_name:\n            batch_df = batch.to_pandas()\n        else:\n            batch_df = batch\n\n        print(f'inserting batch {count}...')\n\n        b_start = time()\n        batch_df.to_sql(name=tb, con=engine, if_exists='append')\n        b_end = time()\n\n        print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n        \n    t_end = time()   \n    print(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \n\n\n\nif __name__ == '__main__':\n    #Parsing arguments \n    parser = argparse.ArgumentParser(description='Loading data from .parquet file link to a Postgres datebase.')\n\n    parser.add_argument('--user', help='Username for Postgres.')\n    parser.add_argument('--password', help='Password to the username for Postgres.')\n    parser.add_argument('--host', help='Hostname for Postgres.')\n    parser.add_argument('--port', help='Port for Postgres connection.')\n    parser.add_argument('--db', help='Databse name for Postgres')\n    parser.add_argument('--tb', help='Destination table name for Postgres.')\n    parser.add_argument('--url', help='URL for .paraquet file.')\n\n    args = parser.parse_args()\n    main(args)\nWe can go to pgAdmin and delete the table by doing:\nDROP TABLE \n    yellow_taxi_data;\nAnd now we don’t have any data, so if we run our script by doing:\npython3 ingest_data.py \\\n    --user=root \\\n    --password=root \\\n    --host=localhost \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --tb=yellow_taxi_trips \\\n    --url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\" \nAnd if we go to pgAdmin and refresh and run:\nSELECT \n    COUNT(1) \nFROM \n    yellow_taxi_trips;\nwe will see all our data again.\n\n\n\nFor this, we need to change our Dockerfile we have in our directory to:\nFROM python:3.9\n\n# We need to install wget to download the parquet file\nRUN apt-get install wget\n# psycopg2 is a postgres db adapter for python: sqlalchemy needs it\nRUN pip install pandas sqlalchemy psycopg2 pyarrow\n\nWORKDIR \\app\n\nCOPY ingest_data.py ingest_data.py \n\nENTRYPOINT [ \"python\", \"ingest_data.py\" ]\nand we can build this doker image by doing:\ndocker build -t taxi_ingest:v001 .\nremember here:\n\nbuild is to build the image from a Dockerfile.\n-t Name and optionally a tag in the name:tag format\n. builds the image in the actual repository\n\nand we run the image by doing:\ndocker run -it \\\n    --network=pg-network \\\n    taxi_ingest:v001 \\\n    --user=root \\\n    --password=root \\\n    --host=pg-database \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --tb=yellow_taxi_trips \\\n    --url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\" \nFew notes:\n\nWe need to provide the network for Docker to find the Postgres container. It goes before the name of the image.\nSince Postgres is running on a separate container, the host argument will have to point to the container name of Postgres.\nYou can drop the table in pgAdmin beforehand if you want, but the script will automatically replace the pre-existing table."
  },
  {
    "objectID": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#terminology",
    "href": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#terminology",
    "title": "Dockerizing",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires.\npsychopg2: is the most popular PostgreSQL database adapter for the Python programming language."
  },
  {
    "objectID": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#useful-links",
    "href": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#useful-links",
    "title": "Dockerizing",
    "section": "",
    "text": "pgAdmin official page."
  },
  {
    "objectID": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#converting-the-notebook-to-a-python-script",
    "href": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#converting-the-notebook-to-a-python-script",
    "title": "Dockerizing",
    "section": "",
    "text": "We could do something like:\njupyter nbconvert --to=script upload-data.ipynb\nto convert the notebook, where we download the data, to a python file. But that’s to slow, so we are just gonna create a new file called ingest_data.py, and we will use the argparse library to pass some cli arguments:\n#Cleaned up version of data-loading.ipynb\nimport argparse, os, sys\nfrom time import time\nimport pandas as pd \nimport pyarrow.parquet as pq\nfrom sqlalchemy import create_engine\n\n\ndef main(params):\n    user = params.user\n    password = params.password\n    host = params.host\n    port = params.port\n    db = params.db\n    tb = params.tb\n    url = params.url\n    \n    # Get the name of the file from url\n    file_name = url.rsplit('/', 1)[-1].strip()\n    print(f'Downloading {file_name} ...')\n    # Download file from url\n    os.system(f'curl {url.strip()} -o {file_name}')\n    print('\\n')\n\n    # Create SQL engine\n    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n\n    # Read file based on csv or parquet\n    if '.csv' in file_name:\n        df = pd.read_csv(file_name, nrows=10)\n        df_iter = pd.read_csv(file_name, iterator=True, chunksize=100000)\n    elif '.parquet' in file_name:\n        file = pq.ParquetFile(file_name)\n        df = next(file.iter_batches(batch_size=10)).to_pandas()\n        df_iter = file.iter_batches(batch_size=100000)\n    else: \n        print('Error. Only .csv or .parquet files allowed.')\n        sys.exit()\n\n\n    # Create the table\n    df.head(0).to_sql(name=tb, con=engine, if_exists='replace')\n\n\n    # Insert values\n    t_start = time()\n    count = 0\n    for batch in df_iter:\n        count+=1\n\n        if '.parquet' in file_name:\n            batch_df = batch.to_pandas()\n        else:\n            batch_df = batch\n\n        print(f'inserting batch {count}...')\n\n        b_start = time()\n        batch_df.to_sql(name=tb, con=engine, if_exists='append')\n        b_end = time()\n\n        print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n        \n    t_end = time()   \n    print(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \n\n\n\nif __name__ == '__main__':\n    #Parsing arguments \n    parser = argparse.ArgumentParser(description='Loading data from .parquet file link to a Postgres datebase.')\n\n    parser.add_argument('--user', help='Username for Postgres.')\n    parser.add_argument('--password', help='Password to the username for Postgres.')\n    parser.add_argument('--host', help='Hostname for Postgres.')\n    parser.add_argument('--port', help='Port for Postgres connection.')\n    parser.add_argument('--db', help='Databse name for Postgres')\n    parser.add_argument('--tb', help='Destination table name for Postgres.')\n    parser.add_argument('--url', help='URL for .paraquet file.')\n\n    args = parser.parse_args()\n    main(args)\nWe can go to pgAdmin and delete the table by doing:\nDROP TABLE \n    yellow_taxi_data;\nAnd now we don’t have any data, so if we run our script by doing:\npython3 ingest_data.py \\\n    --user=root \\\n    --password=root \\\n    --host=localhost \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --tb=yellow_taxi_trips \\\n    --url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\" \nAnd if we go to pgAdmin and refresh and run:\nSELECT \n    COUNT(1) \nFROM \n    yellow_taxi_trips;\nwe will see all our data again."
  },
  {
    "objectID": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#dockerizing-the-script",
    "href": "notes/Data Engineering/week-01/04-dockerizing-the-ingestion-script.html#dockerizing-the-script",
    "title": "Dockerizing",
    "section": "",
    "text": "For this, we need to change our Dockerfile we have in our directory to:\nFROM python:3.9\n\n# We need to install wget to download the parquet file\nRUN apt-get install wget\n# psycopg2 is a postgres db adapter for python: sqlalchemy needs it\nRUN pip install pandas sqlalchemy psycopg2 pyarrow\n\nWORKDIR \\app\n\nCOPY ingest_data.py ingest_data.py \n\nENTRYPOINT [ \"python\", \"ingest_data.py\" ]\nand we can build this doker image by doing:\ndocker build -t taxi_ingest:v001 .\nremember here:\n\nbuild is to build the image from a Dockerfile.\n-t Name and optionally a tag in the name:tag format\n. builds the image in the actual repository\n\nand we run the image by doing:\ndocker run -it \\\n    --network=pg-network \\\n    taxi_ingest:v001 \\\n    --user=root \\\n    --password=root \\\n    --host=pg-database \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --tb=yellow_taxi_trips \\\n    --url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\" \nFew notes:\n\nWe need to provide the network for Docker to find the Postgres container. It goes before the name of the image.\nSince Postgres is running on a separate container, the host argument will have to point to the container name of Postgres.\nYou can drop the table in pgAdmin beforehand if you want, but the script will automatically replace the pre-existing table."
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "title": "Postgres",
    "section": "",
    "text": "Postgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema: A database schema is considered the “blueprint” of a database which describes how the data may relate to other tables or other data models.\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.\n\n\n\n\n\npgcli official page.\n\n\n\n\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\n\n\n\n\n\n\nImportant\n\n\n\nThese values are only meant for testing. Please change them for any serious project.\n\n\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working.\n\n\n\nIn your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database."
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "title": "Postgres",
    "section": "",
    "text": "Postgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema: A database schema is considered the “blueprint” of a database which describes how the data may relate to other tables or other data models.\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL."
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "title": "Postgres",
    "section": "",
    "text": "pgcli official page."
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "title": "Postgres",
    "section": "",
    "text": "PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\n\n\n\n\n\n\nImportant\n\n\n\nThese values are only meant for testing. Please change them for any serious project.\n\n\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working."
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "title": "Postgres",
    "section": "",
    "text": "In your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html",
    "href": "notes/Data Engineering/week-01/08-terraform.html",
    "title": "Terraform",
    "section": "",
    "text": "Terraform: is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\nInfrastructure as Code (IaC): is the managing and provisioning of infrastructure through code instead of through manual processes."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#terminology",
    "href": "notes/Data Engineering/week-01/08-terraform.html#terminology",
    "title": "Terraform",
    "section": "",
    "text": "Terraform: is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\nInfrastructure as Code (IaC): is the managing and provisioning of infrastructure through code instead of through manual processes."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#useful-links",
    "href": "notes/Data Engineering/week-01/08-terraform.html#useful-links",
    "title": "Terraform",
    "section": "Useful links",
    "text": "Useful links\n\nTerraform"
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#what-is-terraform",
    "href": "notes/Data Engineering/week-01/08-terraform.html#what-is-terraform",
    "title": "Terraform",
    "section": "What is Terraform?",
    "text": "What is Terraform?\nTerraform is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently.\n\nWhy Terraform:\n\nSimplicity in keeping track of infraestructure.\nEasier collaboration.\nReproducibility.\nEnsure resources are removed."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#how-terraform-works",
    "href": "notes/Data Engineering/week-01/08-terraform.html#how-terraform-works",
    "title": "Terraform",
    "section": "How Terraform works?",
    "text": "How Terraform works?"
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#what-are-providers",
    "href": "notes/Data Engineering/week-01/08-terraform.html#what-are-providers",
    "title": "Terraform",
    "section": "What are providers?",
    "text": "What are providers?\nCode that allows terraform to communicate to manage resources on, for example:\n\nAWS\nAzure\nGCP\nKubernetes\netc."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#key-terraform-commands",
    "href": "notes/Data Engineering/week-01/08-terraform.html#key-terraform-commands",
    "title": "Terraform",
    "section": "Key Terraform Commands",
    "text": "Key Terraform Commands\n\nterraform init:\n\nInitializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control.\n\nterraform plan:\n\nMatches/previews local changes against a remote state, and proposes an Execution Plan.\n\nterraform apply:\n\nAsks for approval to the proposed plan, and applies changes to cloud.\n\nterraform destroy:\n\nRemoves your stack from the Cloud.\n\n\n\nCreating main.tf file\nYou can go and google terraform google provider click in the first link and copy the use provider code:\n\nthen in the brackets of provider \"google\" put the example usage code.\nOne nice trick is that you can type in the terminal:\nterraform fmt\nto get your main.tf file formatted. Now you can copy and paste your project-id on project."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#initializing-terraform",
    "href": "notes/Data Engineering/week-01/08-terraform.html#initializing-terraform",
    "title": "Terraform",
    "section": "Initializing terraform",
    "text": "Initializing terraform\nNow, we need a way to tell terraform how to use our credentials, for this we can do something like:\nexport GOOGLE_CREDENTIALS='/Users/manuelandersen/Developer/fun/data-eng/week-01/terrademo/keys/my-creds.json'\nyou can check this by doing:\necho $GOOGLE_CREDENTIALS\nNow we can run:\nterraform init\nto get the provider, here:\n\nprovider means the piece of code that terraform is going tou use to talk to GCP.\n\nOur JSON credential files gives us acces to the resources, the provider will be the \"PATH\".\nMore intuitively, the provider gets us to the door, and our JSON credential files opens the door."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#creating-a-storage-bucket-on-gcp",
    "href": "notes/Data Engineering/week-01/08-terraform.html#creating-a-storage-bucket-on-gcp",
    "title": "Terraform",
    "section": "Creating a storage bucket on GCP",
    "text": "Creating a storage bucket on GCP\nFirst you go to:\n\nand you can see that we dont have any bucket:\n\nSo, if you type in google terraform google cloud storage bucket you can acces this link where we can copy and paste the example:\nresource \"google_storage_bucket\" \"auto-expire\" {\n  name          = \"auto-expiring-bucket\"\n  location      = \"US\"\n  force_destroy = true\n\n  lifecycle_rule {\n    condition {\n      age = 1\n    }\n    action {\n      type = \"AbortIncompleteMultipartUpload\"\n    }\n  }\n}\nNow we can run a:\nterraform plan\nand we should get something like this:\n\nAnd now we can deploy this with:\nterraform apply\nit will ask for your approval, you type yes, and then it will create a JSON file in your directory with the bucket. And if we look at our Cloud Strage Buckets we will see that it was created:\n\nWe will use this in a project or whatever. Now we can get rid of this with:\nterraform destroy\nAnd,\n\n\n\n\n\n\nWarning\n\n\n\nuse terraform .gitignore if you are pushin code to github."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#big-query-dataset",
    "href": "notes/Data Engineering/week-01/08-terraform.html#big-query-dataset",
    "title": "Terraform",
    "section": "Big Query Dataset",
    "text": "Big Query Dataset\nSo, lets make some more interesting, like a Big Query Dataset. For this we need to add to our main.tf the next lines:\nresource \"google_bigquery_dataset\" \"demo_dataset\" {\n  dataset_id = \"demo_dataset\"\n}\nhere:\n\ngoogle_bigquery_dataset is the resource.\ndemo_dataset is the name we give to the resource within the terraform files."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#using-variables",
    "href": "notes/Data Engineering/week-01/08-terraform.html#using-variables",
    "title": "Terraform",
    "section": "Using variables",
    "text": "Using variables\nBy convention we would use a file called variables.tf for our variables.\nWe define in our variables.tf the next variables:\nvariable \"project\" {\n    description = \"Project\"\n    default = \"terraform-demo-422523\"\n}\n\nvariable \"region\" {\n    description = \"Region location\"\n    default = \"us-central1\"\n}\n\nvariable \"location\" {\n    description = \"Project Location\"\n    default = \"US\"\n}\n\nvariable \"bq_dataset_name\" {\n    description = \"My BigQuery Dataset Name\"\n    default = \"demo_dataset\"\n}\n\nvariable \"cgs_bucket_name\" {\n    description = \"My Storage Bucket Name\"\n    default = \"terraform-demo-422523-terra-bucket\"\n}\n\nvariable \"gcs_storage_class\" {\n  description = \"Bucket Storage Class\"\n  default = \"STANDARD\"\n}\nso in our main.tf we can write:\nterraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"5.28.0\"\n    }\n  }\n}\n\nprovider \"google\" {\n\n  project = \"terraform-demo-422523\"\n  region  = var.location\n}\n\nresource \"google_storage_bucket\" \"demo-bucket\" {\n  name          = var.cgs_bucket_name\n  location      = var.region\n  force_destroy = true\n\n  lifecycle_rule {\n    condition {\n      age = 1\n    }\n    action {\n      type = \"AbortIncompleteMultipartUpload\"\n    }\n  }\n}\n\nresource \"google_bigquery_dataset\" \"demo_dataset\" {\n  dataset_id = var.bq_dataset_name\n  location = var.location\n}\nthis is way better if we are working in this project with other people and we want to quickly change the value of some variable."
  },
  {
    "objectID": "notes/Data Engineering/week-01/08-terraform.html#changing-the-acces-to-credentials",
    "href": "notes/Data Engineering/week-01/08-terraform.html#changing-the-acces-to-credentials",
    "title": "Terraform",
    "section": "Changing the acces to credentials",
    "text": "Changing the acces to credentials\nRemember that we declare the GOOGLE_CREDENTIALS by exporting them, so to unset them we do:\nunset GOOGLE_CREDENTIALS\nso now if we do terraform plan it should fail. Now we can put the credentials in a variable in our variables.tf file:\nvariable \"credentials\" {\n    description = \"My Credentials\"\n    default = file(\"./keys/my-creds.json\")\n}\nwe also need to include this variable in our main.tf file this way, with a function called file():\nvariable \"credentials\" {\n    description = file(var.credentials)\n    default = file(\"./keys/my-creds.json\")\n}\nAlways remember to do:\nterraform destroy"
  },
  {
    "objectID": "notes/Data Engineering/week-01/06-sql.html",
    "href": "notes/Data Engineering/week-01/06-sql.html",
    "title": "SQL",
    "section": "",
    "text": "SQL: means Structured Query Language. SQL is a standard language for database creation and manipulation."
  },
  {
    "objectID": "notes/Data Engineering/week-01/06-sql.html#terminology",
    "href": "notes/Data Engineering/week-01/06-sql.html#terminology",
    "title": "SQL",
    "section": "",
    "text": "SQL: means Structured Query Language. SQL is a standard language for database creation and manipulation."
  },
  {
    "objectID": "notes/Data Engineering/week-01/06-sql.html#useful-links",
    "href": "notes/Data Engineering/week-01/06-sql.html#useful-links",
    "title": "SQL",
    "section": "Useful links",
    "text": "Useful links\n\nSQL joins: link"
  },
  {
    "objectID": "notes/Data Engineering/week-01/06-sql.html#what-is-sql",
    "href": "notes/Data Engineering/week-01/06-sql.html#what-is-sql",
    "title": "SQL",
    "section": "What is SQL?",
    "text": "What is SQL?\nSo, SQL is a standard language for database creation and manipulation. This part of the lecture is to remember how SQL works. We are gonna use the yellow_taxi_trips data we have in pgAdmin and also we are gonna use this zone data.\nSo basically we have this tables to work with:\n\nyellow_taxi_trips: list of all yelow taxi trips of NYC for January 2021.\nzones: list of zone IDs for pick ups and drop offs.\n\nSo lets say we want to:\n\n1) Question 1\n\nSelects all rows in the yellow_taxi_trips table. If there are more than 100 rows, select only the first 100. We would do:\n\nSELECT\n    *\nFROM\n    yellow_taxi_trips t,\n    zones zpu,\n    zones zdo\nWHERE\n    t.\"PULocationID\" = zpu.\"LocationID\" AND\n    t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\nA few notes on this:\n\nWe select all rows in the trips table. If there are more than 100 rows, select only the first 100.\nWe give aliases to the trips and zones tables for easier access.\nWe replace the IDs inside PULocationID and DOLocationID with the actual zone IDs for pick ups and drop offs.\nWe use double quotes (““) for the column names because in Postgres we need to use them if the column names contains capital letters.\n\n\n\n2) Question 2\n\nWe only want the pickup time, the dop off time, how much the drive was, and the zones. For this we would do:\n\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", '/', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", '/', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM\n    yellow_taxi_trips t JOIN zones zpu\n        ON t.\"PULocationID\" = zpu.\"LocationID\"\n    JOIN zones zdo\n        ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\n\n\n3) Question 3\n\nWe wanna see if there is a NULL in the pickup zone:\n\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    \"PULocationID\",\n    \"DOLocationID\"\nFROM\n    yellow_taxi_trips t\nWHERE\n    \"PULocationID\" is NULL\nLIMIT 100;\n\n\n4) Question 4\n\nCheking for Location ID’s in the zones table not in the yellow_taxi_trips:\n\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    \"PULocationID\",\n    \"DOLocationID\"\nFROM\n    yellow_taxi_trips t\nWHERE\n    \"DOLocationID\" NOT IN (\n        SELECT \"LocationID\" FROM zones\n    )\nLIMIT 100;\n\nBut what if we delete, for example, all the rows in the table zones with LocationID of 142. First, we delete them with:\nDELETE FROM \n    zones \nWHERE \n    \"LocationID\" = 142;\nand now if we run again:\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    \"PULocationID\",\n    \"DOLocationID\"\nFROM\n    yellow_taxi_trips t\nWHERE\n    \"DOLocationID\" NOT IN (\n        SELECT \"LocationID\" FROM zones\n    )\nLIMIT 100;\nwe will get a list of rows with PULocationID of 142:\n\nSo, because this is a more real scenario (where some data is missing), is also more convenient to do a left join to get the pickup time, the dop off time, how much the drive was, and the zones. For this we would do:\nSELECT\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    total_amount,\n    CONCAT(zpu.\"Borough\", '/', zpu.\"Zone\") AS \"pickup_loc\",\n    CONCAT(zdo.\"Borough\", '/', zdo.\"Zone\") AS \"dropoff_loc\"\nFROM\n    yellow_taxi_trips t LEFT JOIN zones zpu\n        ON t.\"PULocationID\" = zpu.\"LocationID\"\n    LEFT JOIN zones zdo\n        ON t.\"DOLocationID\" = zdo.\"LocationID\"\nLIMIT 100;\nLook at what we get if we run this query:\n\nwe can see that we get a / where we don’t know the location.\n\n\n5) Question 5\n\nCounts the amount of records in the yellow_taxi_trips table grouped by day.\n\nSELECT\n    CAST(tpep_pickup_datetime AS DATE) as \"day\",\n    COUNT(1)\nFROM\n    yellow_taxi_trips t\nGROUP BY\n    CAST(tpep_pickup_datetime AS DATE)\nORDER BY \"day\" ASC;\n\n\n6) Question 6\n\nCounts the amount of records in the yellow_taxi_trips table grouped by day, ordered by counts, and display displays them in descending order, so that the day with the highest amount of trips is shown first. Also show the maximum amount that a driver earned in a trip for that day and the maximum passenger count on a single trip for that day.\n\nSELECT\n    CAST(tpep_pickup_datetime AS DATE) as \"day\",\n    COUNT(1) as \"count\",\n    MAX(total_amount) as \"max_amount\",\n    MAX(passenger_count) as \"max_passenger\"\n    FROM\n    yellow_taxi_trips t\nGROUP BY\n    CAST(tpep_pickup_datetime AS DATE)\nORDER BY \"count\" DESC;\n\n\n7) Question 7\n\nCounts the amount of records in the yellow_taxi_trips table grouped by day AND location ID, ordered by day and location id, and displays them in ASCENDING order. Also show the maximum amount that a driver earned in a trip for that day and the maximum passenger count on a single trip for that day.\n\nSELECT\n    CAST(tpep_pickup_datetime AS DATE) as \"day\",\n    \"DOLocationID\",\n    COUNT(1) as \"count\",\n    MAX(total_amount) as \"max_amount\",\n    MAX(passenger_count) as \"max_passenger\"\nFROM\n    yellow_taxi_trips t\nGROUP BY\n    1, 2\nORDER BY\n    \"day\" ASC,\n    \"DOLocationID\" ASC;"
  },
  {
    "objectID": "notes/Data Engineering/week-03/01-data-warehouse.html",
    "href": "notes/Data Engineering/week-03/01-data-warehouse.html",
    "title": "Data Warehouse",
    "section": "",
    "text": "OLAP vs OLTP\n\nOLTP: Online Transaction Processing.\nOLAP: Online Analytical Processing.\n\nOLTP are like classic databases and OLAP are more for advanced data analytics.\n\n\n\n\n\n\n\n\n\nOLTP\nOLAP\n\n\n\n\nPurpose\nControl and run essential business operations in real time\nPlan, solve problems, support decisions, discover hidden insights\n\n\nData updates\nShort, fast updates initiated by user\nData periodically refreshed with scheduled, long-running batch jobs\n\n\nDatabase design\nNormalized databases for efficiency\nDenormalized databases for analysis\n\n\nSpace requirements\nGenerally small if historical data is archived\nGenerally large due to aggregating large datasets\n\n\nBackup and recovery\nRegular backups required to ensure business continuity and meet legal and governance requirements\nLost data can be reloaded from OLTP database as needed in lieu of regular backups\n\n\nProductivity\nIncreases productivity of end users\nIncreases productivity of business managers, data analysts and executives\n\n\nData view\nLists day-to-day business transactions\nMulti-dimensional view of enterprise data\n\n\nUser examples\nCustomer-facing personnel, clerks, online shoppers\nKnowledge workers such as data analysts, business analysts and executives\n\n\n\n\n\nWhat is a Data Warehouse?\nA Data Warehouse is an OLAP solution used for reporting and data analysis, and generally consist of a various data sources.\n\n\n\nBigQuery\nBigQuery is a Data Warehouse solution offered by Google Cloud Platform. Some of is advantages:\n\nIs a serverless data warehouse. There are no servers to manage or database software to install; this is managed by Google and it’s transparent to the customers.\nIs scalable and has high availability. Google takes care of the underlying software and infrastructure.\nHas built-in features like Machine Learning, Geospatial Analysis and Business Intelligence among others.\nIt maximizes flexibility by separating data analysis and storage in different compute engines, thus allowing the customers to budget accordingly and reduce costs.\n\n\n\nPartitions\nBigQuery tables can be partitioned into multiple smaller tables. For example, if we often filter queries based on date, we could partition a table based on date so that we only query a specific sub-table based on the date we’re interested in.\nPartition tables are very useful to improve performance and reduce costs, because BigQuery will not process as much data per query.\nYou may partition a table by:\n\nTime-unit column: tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table.\nIngestion time: tables are partitioned based on the timestamp when BigQuery ingests the data.\nInteger range: tables are partitioned based on an integer column.\n\nFor Time-unit and Ingestion time columns, the partition may be daily (the default option), hourly, monthly or yearly.\nTo create a partition of our data set in BigQuery we do:\nCREATE OR REPLACE TABLE\n  lucky-history-422623-a7.ny_taxi.yellow_cab_data_partitioned\nPARTITION BY\n  DATE(tpep_pickup_datetime) AS\nSELECT \n  *\nFROM \n  lucky-history-422623-a7.ny_taxi.yellow_cab_data_external\nIf we wanna look into the partitions:\nSELECT\n  table_name, \n  partition_id,\n  total_rows\nFROM\n  `ny_taxi.INFORMATION_SCHEMA.PARTITIONS`\nWHERE\n  table_name = 'yellow_cab_data_partitioned'\nORDER BY\n  total_rows DESC;\n\n\nClustering\nClustering consists of rearranging a table based on the values of its columns so that the table is ordered according to any criteria. Clustering can be done based on one or multiple columns up to 4; the order of the columns in which the clustering is specified is important in order to determine the column priority.\nClustering may improve performance and lower costs on big datasets for certain types of queries, such as queries that use filter clauses and queries that aggregate data.\nIf we wanna cluster, for example, by vendorid we can do:\nCREATE OR REPLACE TABLE\n  lucky-history-422623-a7.ny_taxi.yellow_cab_data_partitioned_clustered\nPARTITION BY\n  DATE(tpep_pickup_datetime) \nCLUSTER BY vendorid AS \nSELECT \n  *\nFROM \n  lucky-history-422623-a7.ny_taxi.yellow_cab_data_external"
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html",
    "title": "Orchestration",
    "section": "",
    "text": "A large part of data engineering is extracting, transforming and loading data between sources. Orchestration is a process of dependency management, facilitated trough automation. The data orchestrator manages scheduling, triggering, monitoring, adnd even resource allocation for you data engineering workflows. Every workflow requires sequential steps:\n\nSteps == tasks\nWorkflows == DAG’s (directed acyclic graphs) or Pipeline\nData orchestrator:"
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#what-is-orchestration",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#what-is-orchestration",
    "title": "Orchestration",
    "section": "",
    "text": "A large part of data engineering is extracting, transforming and loading data between sources. Orchestration is a process of dependency management, facilitated trough automation. The data orchestrator manages scheduling, triggering, monitoring, adnd even resource allocation for you data engineering workflows. Every workflow requires sequential steps:\n\nSteps == tasks\nWorkflows == DAG’s (directed acyclic graphs) or Pipeline\nData orchestrator:"
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#what-is-mage",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#what-is-mage",
    "title": "Orchestration",
    "section": "What is Mage?",
    "text": "What is Mage?\nOpen-source data pipeline tool for transforming and integrating data. The modern replacement for Airflow.\nIt consist of:\n\nProjects: forms the basis for all the work you can do in Mage - you can think of it like a Github repo. It contains the code for all of your pipelines, blocks and other assets.\nPipelines: is a workflow that executes some data operation - maybe extracting, transforming and loading data from an API.\nBlocks: is a file that can be executed independently or within a pipeline."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configure-mage",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configure-mage",
    "title": "Orchestration",
    "section": "Configure Mage",
    "text": "Configure Mage\nFirst, let’s clone this repo:\ngit clone https://github.com/mage-ai/mage-zoomcamp\nyou go to the repo with cd and then rename dev.env to .env — this will ensure the file is not committed to Git by accident, since it will contain credentials in the future.\nNow we can build the container:\ndocker compose build\nwich has mageai and postgres. Now we can run the container by doing:\ndocker compose up\nand then in our browser we go to localhost:6789 we will get something like this:\n\nwich is the mage instance."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#a-simple-pipeline",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#a-simple-pipeline",
    "title": "Orchestration",
    "section": "A simple Pipeline",
    "text": "A simple Pipeline\nGo to the Pipelines tab, wich gives you an overview of all of the pipelines that we have. The docker compose will have created a Project in mage called magic-zoomcamp, wich will have an example_pipeline. Here’s how that pipeline looks:\n\nyou can play with it, but we are gonna use this to make a more interesting pipeline wich will send the data to postgres."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configuring-postgres",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configuring-postgres",
    "title": "Orchestration",
    "section": "Configuring Postgres",
    "text": "Configuring Postgres\nIn order to write things into postgres we need to first connect to postgres, for this we are gonna do it the simple way, with the mage GUI. Go to the Files tab and then open the io_config.yaml file, which is were we manage our conections. Here we have a default profile, but let’s create a new one called dev.\ndev:\n  # PostgresSQL\n  POSTGRES_CONNECT_TIMEOUT: 10\n  POSTGRES_DBNAME: \"{{ env_var('POSTGRES_DBNAME') }}\"\n  POSTGRES_SCHEMA: \"{{ env_var('POSTGRES_SCHEMA') }}\"\n  POSTGRES_USER: \"{{ env_var('POSTGRES_USER') }}\"\n  POSTGRES_PASSWORD: \"{{ env_var('POSTGRES_PASSWORD') }}\"\n  POSTGRES_HOST: \"{{ env_var('POSTGRES_HOST') }}\"\n  POSTGRES_PORT: \"{{ env_var('POSTGRES_PORT') }}\"\nNote that we are pulling the environment variables trough jinja templating, wich is the way we pull variables in mage.\nWe can now test this by making a new pipeline called test_config and make a SQLData Loader block inside this called test_postgres, where we can define a PostgreSQL connection and the dev profile. We can run SQL code here wich will be run in Postgres, and we will be sure that connection is made:\nSELECT 1;"
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#etl-api-to-postgres",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#etl-api-to-postgres",
    "title": "Orchestration",
    "section": "ETL: API to Postgres",
    "text": "ETL: API to Postgres\nSo, its not going to be an actual API, just a compress csv, but the idea is pretty similar. First, lets create a new Standard (batch) pipeline called api_to_postgres and we are gonna edit the pipeline and create a Data loader for Python, and use an API template, and lets called it load_api_data.\n\n\n\n\n\n\nBest Practice\n\n\n\nAlways map data types, in order to manage memory consumption.\n\n\nWe use this python script:\nimport io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz'\n    taxi_dtypes = {\n        'VendorID': pd.Int64Dtype(),\n        'passenger_count': pd.Int64Dtype(),\n        'trip_distance': float,\n        'RatecodeID': pd.Int64Dtype(),\n        'store_and_fwd_flag': str,\n        'PULocationID': pd.Int64Dtype(),\n        'DOLocationID': pd.Int64Dtype(),\n        'payment_type': pd.Int64Dtype(),\n        'fare_amount': float,\n        'extra': float,\n        'mta_tax': float,\n        'tip_amount': float,\n        'tolls_amount': float,\n        'improvement_surcharge': float,\n        'total_amount': float,\n        'congestion_surcharge': float \n    }\n\n    parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n\n    return pd.read_csv(url, sep=\",\", compression=\"gzip\", dtype=taxi_dtypes, parse_dates=parse_dates)\n\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\nThen we are gona transform the data, for this we create a generic Transformer block called transform_taxi_data.\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\n\n@transformer\ndef transform(data, *args, **kwargs):\n    # Specify your transformation logic here\n\n    zero_passengers_df = data[data['passenger_count'].isin([0])]\n    zero_passengers_count = zero_passengers_df['passenger_count'].count()\n    non_zero_passengers_df = data[data['passenger_count'] &gt; 0]\n    print(f'Preprocessing: records with zero passengers: {zero_passengers_count}')\n\n    return non_zero_passengers_df\n\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'\nNow, we are gonna export the data to postgres using the Data exporter block.\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -&gt; None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'ny_taxi'  # Specify the name of the schema to export data to\n    table_name = 'yellow_cab_data'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'dev'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\nAnd now we can test our load by doing a SQL Data loader block and do:\nSELECT * \nFROM \n    ny_taxi.yellow_cab_data \nLIMIT \n    10\nand we should see our data."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configuring-gcp",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#configuring-gcp",
    "title": "Orchestration",
    "section": "Configuring GCP",
    "text": "Configuring GCP\nFirst of all, got to Google Cloud Platform and create a Bucket, then go and create a Service Account to allow Mage to connect to GCP. You can copy the JSON keys to your working folder.\nNow, in Mage, we can go to the terminal and look for the Path of the JSON keys and paste them in the GOOGLE_SERVICE_ACC_KEY_FILPATH inside the io_config.yaml file. We can test the connection by running the test_postgres pipeline, with BigQuery connection and with default Profile.\n\n\n\n\n\n\nNote\n\n\n\nCheck the “Use Raw SQL” block, otherwise you will get a 404 error.\n\n\nYou can then run the example_pipeline to download the titanic_clean.csv into your local directory. Load this data into your bucket, and then delete test_postgres and create a new data loader called test_gcs. If you ran this, you will see that you can acces the data trough BigQuery."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#api-to-gcs",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#api-to-gcs",
    "title": "Orchestration",
    "section": "API to GCS",
    "text": "API to GCS\nLeta create a new pipeline, and a cool thing about Mage its that we can reutilize code simply by dragging and dropping in the pipeline. So, lets reutilize the load_api_data Data loader and the transform_taxi_data Transformer. Then we create a Data Exporter called taxi_to_gcs_parquet:\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -&gt; None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'mage-zoomcamp-manuel-andersen'\n    object_key = 'nyc_taxi_data.parquet'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\nwe the run this pipeline by clicking Execute with all upstream blocks and we should see our data exported in parquet file to our bucket.\nThe next natural question is: If we have a larger dataset, do we want to export it into a single parquet file? No, so we are gonna write this into a partitioned parquet file structure."
  },
  {
    "objectID": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#partitioned-parquet-exporter",
    "href": "notes/Data Engineering/week-02/01-intro-to-orchestration.html#partitioned-parquet-exporter",
    "title": "Orchestration",
    "section": "Partitioned parquet exporter",
    "text": "Partitioned parquet exporter\nLets create a Data exporter called taxi_to_gcs_partitioned_parquet and lets connec it to the upstream Transformer block (we can just drag the connection with the Tree associated to the pipeline in Mage).\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/home/src/lucky-history-422623-a7-b0f9ecbd2a23.json\"\n\nbucket_name = 'mage-zoomcamp-manuel-andersen'\nproject_id = 'lucky-history-422623-a7'\n\ntable_name = \"nyc_taxi_data\"\n\nroot_path = f'{bucket_name}/{table_name}'\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    data['tpep_pickup_date'] = data['tpep_pickup_datetime'].dt.date\n\n    table = pa.Table.from_pandas(data)\n\n    gcs = pa.fs.GcsFileSystem()\n\n    pq.write_to_dataset(\n        table,\n        root_path=root_path,\n        partition_cols=['tpep_pickup_date'],\n        filesystem=gcs\n    )\nhere we are partitioning by date, so if you run this and then look at your bucket you should see a folder called nyc_taxi_data wich inside has a bunch of other folders, each one of those representing a date, inside this ones are the partitioned data."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Padel Scraper\n\n\nA tool that collects data from the padelfip website.\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "I’d love to hear from you! ",
    "section": "",
    "text": "I’d love to hear from you! \nYou can use this form to contact me about speaking engagements, collaborations, consulting or simply to say hello."
  },
  {
    "objectID": "projects/padel-scrapy/index.html#motivation",
    "href": "projects/padel-scrapy/index.html#motivation",
    "title": "Padel Scraper",
    "section": "Motivation",
    "text": "Motivation\nThe idea for this project began when I wanted to perform some analysis of padel data, knowing that the sport was growing rapidly. When I started searching for datasets, I quickly realized there weren’t many available, and the ones I found were mainly videos. So, I thought, “Okay, time to create a dataset myself—how hard can it be?” Spoiler alert: it was hard.\nSo, I started investigating how others had approached similar projects and came across a project called transfermarkt-datasets. The authors had done exactly what I wanted to do, but they were scraping data from the Transfermarkt website. They extracted the data using a package called scrapy, and I really liked the way they structured their project. So, I decided to learn a bit about the Scrapy library and try to scrape the padelfip website using it."
  },
  {
    "objectID": "projects/padel-scrapy/index.html#first-fase",
    "href": "projects/padel-scrapy/index.html#first-fase",
    "title": "Padel Scraper",
    "section": "First Fase",
    "text": "First Fase\nFirst, I quickly looked into a tutorial to learn the basics of the library. I used this tutorial and probably watched just the first 30 minutes before I started experimenting on my own.\nThe first thing I tried to scrape was the player’s ranking. The page had three distinct sections:\n\nsection_sliderHomeWrap: this section appeared to be only for the top 10 players.\nsection_playerGridWrap: this section covered players ranked 11 to 20.\nplayer-table: this section included players ranked 21 to 40, with the option to load more players.\n\nThe second thing I noticed was that each player had their own URL leading to data about their participation in every tournament they played. So, the data for each player ended up looking something like this:\n{\n        \"ranking\": \"1\",\n        \"name\": \"Arturo Coello\",\n        \"country\": \"ESP\",\n        \"points\": \"13750\",\n        \"paired_with\": \"Agustin Tapia\",\n        \"position\": \"Right\",\n        \"birth_date\": \"08/03/2002\",\n        \"height\": \"1,90\",\n        \"place_of_birth\": \"Valladolid\",\n        \"residence\": null,\n        \"matches_played\": \"58\",\n        \"matches_won\": \"52\",\n        \"matches_lost\": \"6\",\n        \"consecutive_victories\": \"18\",\n        \"effectiveness\": \"89.66%\",\n        \"old_position\": null,\n        \"points_per_tournament\": [\n            {\n                \"tournament_name\": \"MALAGA PREMIER PADEL P1\",\n                \"category\": \"PREMIER PADEL P1\",\n                \"date\": \"06/07/2024\",\n                \"round_reached\": \" Winner\",\n                \"points\": \"1000\"\n            },\n            {\n                \"tournament_name\": \"GENOVA PREMIER PADEL P2\",\n                \"category\": \"PREMIER PADEL P2\",\n                \"date\": \"30/06/2024\",\n                \"round_reached\": \"Finals\",\n                \"points\": \"300\"\n            },\n            .\n            .\n            .\n            .\n            .\n            {\n                \"tournament_name\": \"Doha\",\n                \"category\": \"PREMIER PADEL MAJOR\",\n                \"date\": \"02/04/2022\",\n                \"round_reached\": null,\n                \"points\": \"360\"\n            }\n        ]\n    }"
  },
  {
    "objectID": "projects/padel-scrapy/index.html#second-fase",
    "href": "projects/padel-scrapy/index.html#second-fase",
    "title": "Padel Scraper",
    "section": "Second Fase",
    "text": "Second Fase\nThe second thing i planned to scrape was the tournaments page. For each tournament, besides all the tournament information itself, it was important to also scrape the tournament’s URL of stats and the number of days the tournament was played. Here’s a quick look at how the scraped data turned out:\n{\n        \"year\": 2024,\n        \"month\": \"Septiembre\",\n        \"event_name\": \"ROTTERDAM PREMIER PADEL P1\",\n        \"event_place\": \"Rotterdam - Netherlands\",\n        \"event_date\": \"07/09/2024\\n-15/09/2024\",\n        \"gender\": \"Female/Male\",\n        \"qualification_date\": [],\n        \"prize_money\": \"€ 470.000\",\n        \"club_info\": null,\n        \"days_played\": null,\n        \"url\": null\n    },\n    .\n    .\n    .\n    .\n    .\n    {\n        \"year\": 2023,\n        \"month\": \"Septiembre\",\n        \"event_name\": \"PARIS MAJOR PREMIER PADEL\",\n        \"event_place\": \"Paris - France\",\n        \"event_date\": \"03/09/2023\\n-10/09/2023\",\n        \"gender\": \"Female/Male\",\n        \"qualification_date\": [],\n        \"prize_money\": \"EUR 525000\",\n        \"club_info\": null,\n        \"days_played\": 8,\n        \"url\": \"https://widget.matchscorerlive.com/screen/resultsbyday/FIP-2023-3603/8?t=tol\"\n    }"
  },
  {
    "objectID": "projects/padel-scrapy/index.html#third-fase",
    "href": "projects/padel-scrapy/index.html#third-fase",
    "title": "Padel Scraper",
    "section": "Third Fase",
    "text": "Third Fase\nThe last part was the tricky one: scraping the game stats for every tournament. Each tournament had a different URL where the statistics for every match played during the tournament were listed. To run the gamespider, I assumed that this URL would be passed as an argument along with the days_played. Here’s a quick look at how the data turned out:\n[\n    {\n        \"tournament\": \"LOTTO BRUSSELS PREMIER PADEL P2 PRESENTED BY BELFIUS\",\n        \"date\": \"Sun, Apr 28\",\n        \"court\": \"CENTRAL COURT\",\n        \"gender\": \"Women\",\n        \"round\": \"F\",\n        \"time\": \"01:51\",\n        \"winner_player_name\": \"D. Brea Senesi B. Gonzalez Fernandez\",\n        \"loser_player_name\": \"G. Triay Pons C. Fernandez Sanchez\",\n        \"left_players\": [\n            \"TRIAY PONS\",\n            \"FERNANDEZ SANCHEZ\"\n        ],\n        \"right_players\": [\n            \"BREA SENESI\",\n            \"GONZALEZ FERNANDEZ\"\n        ],\n        \"scores\": [\n            \"4-6\",\n            \"4-6\"\n        ],\n        \"left_match_pctg_total_points_won\": \"47%\",\n        \"left_match_pctg_numbers_total_points_won\": \"61/130\",\n        \"right_match_pctg_total_points_won\": \"53%\",\n        \"right_match_pctg_numbers_total_points_won\": \"69/130\",\n        \"left_match_pctg_break_points_converted\": \"29%\",\n        \"left_match_pctg_numbers_break_points_converted\": \"2/7\",\n        \"right_match_pctg_break_points_converted\": \"33%\",\n        \"right_match_pctg_numbers_break_points_converted\": \"4/12\",\n        \"left_match_pctg_first_serve_point_won\": \"57%\",\n        \"left_match_pctg_numbers_first_serve_point_won\": \"38/67\",\n        \"right_match_pctg_first_serve_point_won\": \"68%\",\n        \"right_match_pctg_numbers_first_serve_point_won\": \"32/47\",\n        \"left_match_pctg_second_serve_point_won\": \"0%\",\n        \"left_match_pctg_numbers_second_serve_point_won\": \"0/4\",\n        \"right_match_pctg_second_serve_point_won\": \"33%\",\n        \"right_match_pctg_numbers_second_serve_point_won\": \"4/12\",\n        \"left_match_pctg_first_return_points_won\": \"32%\",\n        \"left_match_pctg_numbers_first_return_points_won\": \"15/47\",\n        \"right_match_pctg_first_return_points_won\": \"43%\",\n        \"right_match_pctg_numbers_first_return_points_won\": \"29/67\",\n        \"left_match_pctg_second_return_points\": \"67%\",\n        \"left_match_pctg_numbers_second_return_points\": \"8/12\",\n        \"right_match_pctg_second_return_points\": \"100%\",\n        \"right_match_pctg_numbers_second_return_points\": \"4/4\",\n        \"left_match_pctg_total_serve_points_won\": \"54%\",\n        \"left_match_pctg_numbers_total_serve_points_won\": \"38/71\",\n        \"right_match_pctg_total_serve_points_won\": \"61%\",\n        \"right_match_pctg_numbers_total_serve_points_won\": \"36/59\",\n        \"left_match_pctg_total_return_points_won\": \"39%\",\n        \"left_match_pctg_numbers_total_return_points_won\": \"23/59\",\n        \"right_match_pctg_total_return_points_won\": \"46%\",\n        \"right_match_pctg_numbers_total_return_points_won\": \"33/71\",\n        \"left_1_set_pctg_total_points_won\": \"47%\",\n        \"left_1_set_pctg_numbers_total_points_won\": \"34/73\",\n        \"right_1_set_pctg_total_points_won\": \"53%\",\n        \"right_1_set_pctg_numbers_total_points_won\": \"39/73\",\n        \"left_1_set_pctg_break_points_converted\": \"50%\",\n        \"left_1_set_pctg_numbers_break_points_converted\": \"1/2\",\n        \"right_1_set_pctg_break_points_converted\": \"22%\",\n        \"right_1_set_pctg_numbers_break_points_converted\": \"2/9\",\n        \"left_1_set_pctg_first_serve_point_won\": \"59%\",\n        \"left_1_set_pctg_numbers_first_serve_point_won\": \"24/41\",\n        \"right_1_set_pctg_first_serve_point_won\": \"71%\",\n        \"right_1_set_pctg_numbers_first_serve_point_won\": \"15/21\",\n        \"left_1_set_pctg_second_serve_point_won\": \"0%\",\n        \"left_1_set_pctg_numbers_second_serve_point_won\": \"0/4\",\n        \"right_1_set_pctg_second_serve_point_won\": \"43%\",\n        \"right_1_set_pctg_numbers_second_serve_point_won\": \"3/7\",\n        \"left_1_set_pctg_first_return_points_won\": \"29%\",\n        \"left_1_set_pctg_numbers_first_return_points_won\": \"6/21\",\n        \"right_1_set_pctg_first_return_points_won\": \"41%\",\n        \"right_1_set_pctg_numbers_first_return_points_won\": \"17/41\",\n        \"left_1_set_pctg_second_return_points\": \"57%\",\n        \"left_1_set_pctg_numbers_second_return_points\": \"4/7\",\n        \"right_1_set_pctg_second_return_points\": \"100%\",\n        \"right_1_set_pctg_numbers_second_return_points\": \"4/4\",\n        \"left_1_set_pctg_total_serve_points_won\": \"53%\",\n        \"left_1_set_pctg_numbers_total_serve_points_won\": \"24/45\",\n        \"right_1_set_pctg_total_serve_points_won\": \"64%\",\n        \"right_1_set_pctg_numbers_total_serve_points_won\": \"18/28\",\n        \"left_1_set_pctg_total_return_points_won\": \"36%\",\n        \"left_1_set_pctg_numbers_total_return_points_won\": \"10/28\",\n        \"right_1_set_pctg_total_return_points_won\": \"47%\",\n        \"right_1_set_pctg_numbers_total_return_points_won\": \"21/45\",\n        \"left_2_set_pctg_total_points_won\": \"47%\",\n        \"left_2_set_pctg_numbers_total_points_won\": \"27/57\",\n        \"right_2_set_pctg_total_points_won\": \"53%\",\n        \"right_2_set_pctg_numbers_total_points_won\": \"30/57\",\n        \"left_2_set_pctg_break_points_converted\": \"20%\",\n        \"left_2_set_pctg_numbers_break_points_converted\": \"1/5\",\n        \"right_2_set_pctg_break_points_converted\": \"67%\",\n        \"right_2_set_pctg_numbers_break_points_converted\": \"2/3\",\n        \"left_2_set_pctg_first_serve_point_won\": \"54%\",\n        \"left_2_set_pctg_numbers_first_serve_point_won\": \"14/26\",\n        \"right_2_set_pctg_first_serve_point_won\": \"65%\",\n        \"right_2_set_pctg_numbers_first_serve_point_won\": \"17/26\",\n        \"left_2_set_pctg_second_serve_point_won\": \"0%\",\n        \"left_2_set_pctg_numbers_second_serve_point_won\": \"0/0\",\n        \"right_2_set_pctg_second_serve_point_won\": \"20%\",\n        \"right_2_set_pctg_numbers_second_serve_point_won\": \"1/5\",\n        \"left_2_set_pctg_first_return_points_won\": \"35%\",\n        \"left_2_set_pctg_numbers_first_return_points_won\": \"9/26\",\n        \"right_2_set_pctg_first_return_points_won\": \"46%\",\n        \"right_2_set_pctg_numbers_first_return_points_won\": \"12/26\",\n        \"left_2_set_pctg_second_return_points\": \"80%\",\n        \"left_2_set_pctg_numbers_second_return_points\": \"4/5\",\n        \"right_2_set_pctg_second_return_points\": \"0%\",\n        \"right_2_set_pctg_numbers_second_return_points\": \"0/0\",\n        \"left_2_set_pctg_total_serve_points_won\": \"54%\",\n        \"left_2_set_pctg_numbers_total_serve_points_won\": \"14/26\",\n        \"right_2_set_pctg_total_serve_points_won\": \"58%\",\n        \"right_2_set_pctg_numbers_total_serve_points_won\": \"18/31\",\n        \"left_2_set_pctg_total_return_points_won\": \"42%\",\n        \"left_2_set_pctg_numbers_total_return_points_won\": \"13/31\",\n        \"right_2_set_pctg_total_return_points_won\": \"46%\",\n        \"right_2_set_pctg_numbers_total_return_points_won\": \"12/26\",\n        \"left_match_longest_points_won_streak\": \"6\",\n        \"right_match_longest_points_won_streak\": \"8\",\n        \"left_match_average_point_duration_in_seconds\": \"0\",\n        \"right_match_average_point_duration_in_seconds\": \"0\",\n        \"left_match_aces\": \"0\",\n        \"right_match_aces\": \"0\",\n        \"left_match_double_faults\": \"0\",\n        \"right_match_double_faults\": \"0\",\n        \"left_match_services_games_played\": \"10\",\n        \"right_match_services_games_played\": \"10\",\n        \"left_match_return_games_played\": \"10\",\n        \"right_match_return_games_played\": \"10\",\n        \"left_1_set_longest_points_won_streak\": \"6\",\n        \"right_1_set_longest_points_won_streak\": \"5\",\n        \"left_1_set_average_point_duration_in_seconds\": \"0\",\n        \"right_1_set_average_point_duration_in_seconds\": \"0\",\n        \"left_1_set_aces\": \"0\",\n        \"right_1_set_aces\": \"0\",\n        \"left_1_set_double_faults\": \"0\",\n        \"right_1_set_double_faults\": \"0\",\n        \"left_1_set_services_games_played\": \"5\",\n        \"right_1_set_services_games_played\": \"5\",\n        \"left_1_set_return_games_played\": \"5\",\n        \"right_1_set_return_games_played\": \"5\",\n        \"left_2_set_longest_points_won_streak\": \"5\",\n        \"right_2_set_longest_points_won_streak\": \"4\",\n        \"left_2_set_average_point_duration_in_seconds\": \"0\",\n        \"right_2_set_average_point_duration_in_seconds\": \"0\",\n        \"left_2_set_aces\": \"0\",\n        \"right_2_set_aces\": \"0\",\n        \"left_2_set_double_faults\": \"0\",\n        \"right_2_set_double_faults\": \"0\",\n        \"left_2_set_services_games_played\": \"5\",\n        \"right_2_set_services_games_played\": \"5\",\n        \"left_2_set_return_games_played\": \"5\",\n        \"right_2_set_return_games_played\": \"5\"\n    }\n]\nSo… that’s a lot of data. A quick explanation: the stats page displays one pair of players on the left and the opposing pair on the right, something like this:\n\nSo the scores, the couples and all the associates statistica have a prefix left_ or right_ depending for which couple they belong."
  },
  {
    "objectID": "projects/padel-scrapy/index.html#improvements-needed",
    "href": "projects/padel-scrapy/index.html#improvements-needed",
    "title": "Padel Scraper",
    "section": "Improvements Needed",
    "text": "Improvements Needed\nThere are a few things I still need to work on:\n\nThe playerspider currently only scrapes the men’s ranking, but it should be fairly straightforward to add the logic to scrape both the men’s and women’s rankings.\nThere’s an issue with the basic data in tournamentspider, specifically with the qualification_data. Some pages have this information in a box labeled “Cualificación,” while others include it in the main draw box labeled “Cuadro.” I’ll need to adapt the code to handle this variation."
  },
  {
    "objectID": "projects/padel-scrapy/index.html#whats-next",
    "href": "projects/padel-scrapy/index.html#whats-next",
    "title": "Padel Scraper",
    "section": "What’s Next?",
    "text": "What’s Next?\nMy next plan is to create a project called padel-dataset that will extract all the data, prepare it for publication, and then either upload it to a Kaggle dataset or store it in an S3 bucket—I haven’t decided yet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n      Manuel\n      Andersen\n    ",
    "section": "",
    "text": "I’m an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\n    \n\n    \n    About Me →"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "\n      Manuel\n      Andersen\n    ",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\nA better Harry Potter Book\n\n\nWhat would happen if the book had more points of view?\n\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-projects",
    "href": "index.html#latest-projects",
    "title": "\n      Manuel\n      Andersen\n    ",
    "section": "Latest Projects",
    "text": "Latest Projects\n\n\n\n\n\n\n\n\n\n\nPadel Scraper\n\n\nA tool that collects data from the padelfip website.\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#lo-último-en-español",
    "href": "index.html#lo-último-en-español",
    "title": "\n      Manuel\n      Andersen\n    ",
    "section": "Lo último en Español",
    "text": "Lo último en Español\n\n\n\n\n\nNo matching items"
  }
]