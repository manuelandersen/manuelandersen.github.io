[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manuel Andersen",
    "section": "",
    "text": "I’m an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\nI live in Chile, which is in South America, so some parts of this blog may be in English and others in Spanish."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Manuel Andersen",
    "section": "",
    "text": "I’m an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\nI live in Chile, which is in South America, so some parts of this blog may be in English and others in Spanish."
  },
  {
    "objectID": "projects/HP/index.html",
    "href": "projects/HP/index.html",
    "title": "A better Harry Potter Book",
    "section": "",
    "text": "The Time In Between: This is a look at the developing relationship between Hermione and Ron set during the Deathly Hallows.\nHermione Granger Series: This fan rewrote the first four books from Hermione’s point of view only. The links to each storie:\n\nHermione Granger and the Philosopher’s Stone\nHermione Granger and the Chamber of Secrets\nHermione Granger and the Prisoner of Azkaban\nHermione Granger and the Goblet of Fire"
  },
  {
    "objectID": "projects/HP/index.html#some-links-to-fan-made-stories",
    "href": "projects/HP/index.html#some-links-to-fan-made-stories",
    "title": "A better Harry Potter Book",
    "section": "",
    "text": "The Time In Between: This is a look at the developing relationship between Hermione and Ron set during the Deathly Hallows.\nHermione Granger Series: This fan rewrote the first four books from Hermione’s point of view only. The links to each storie:\n\nHermione Granger and the Philosopher’s Stone\nHermione Granger and the Chamber of Secrets\nHermione Granger and the Prisoner of Azkaban\nHermione Granger and the Goblet of Fire"
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "title": "Postgres",
    "section": "",
    "text": "Postgres:\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema:\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.\n\n\n\n\n\npgcli official page.\n\n\n\n\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\nIMPORTANT: These values are only meant for testing. Please change them for any serious project.\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\n\n\nscreenshot\n\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\n\n\nscreenshot\n\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working.\n\n\n\nIn your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\n\n\nscreenshot\n\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\n\n\nscreenshot\n\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "title": "Postgres",
    "section": "",
    "text": "Postgres:\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema:\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "title": "Postgres",
    "section": "",
    "text": "pgcli official page.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "title": "Postgres",
    "section": "",
    "text": "PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\nIMPORTANT: These values are only meant for testing. Please change them for any serious project.\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\n\n\nscreenshot\n\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\n\n\nscreenshot\n\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "title": "Postgres",
    "section": "",
    "text": "In your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\n\n\nscreenshot\n\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\n\n\nscreenshot\n\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "title": "Docker",
    "section": "Terminology",
    "text": "Terminology\n\nContainer: environment that runs an applications that is not dependent on the OS. Kind of like a lightweight VM. Containers are stateless; if you need to update the components inside, create another container instead.\nImage: template to create a container. Its components are defined by a Dockerfile.\nVolume: storage area detached from the container for maintaining state.\nForeground/interactive vs background/detached: a detached container runs in the background whereas an interactive container will usually have a terminal of some sort for interacting with.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "title": "Docker",
    "section": "Useful links",
    "text": "Useful links\n\nCheatsheet",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "title": "Docker",
    "section": "What is Docker?",
    "text": "What is Docker?\nDocker is a containerization software that allows us to isolate software in a similar way to virtual machines but in a much leaner way.\nDocker delivers software in packages called containers. These containers are isolated from one another. If we run a data pipeline inside a container it is virtually isolated from the rest of the things running on the computer.\nA Docker image is a snapshot of a container that we can define to run our software, or in this case our data pipelines. By exporting our Docker images to Cloud providers such as Amazon Web Services or Google Cloud Platform we can run our containers there.\n\n\n\nscreenshot\n\n\nWe can run the container we have created through the docker image where we have specified and configured the environment beyond the host computer and essentially everywhere like - Google Cloud (Kubernetes), AWS Batch etc.\nDocker Image ensures reproducibility regardless of the machine as the images are identical. We can specify OS, programming languages, packages, database type, tools etc. This solves the problem of “Works on my computer but NoT iN yOuRs.”\nSo, the advantages of working with Docker?\n\nLocal Experiments: Helps to run things locally like your database also helps with testing like integration testing.\nIntegration Tests (CI/CD).\nReproducibility: Docker makes things run everywhere regardless of what you are using.\nRunning Pipelines on the Cloud (AWS Batch, Kubernetes Jobs).\nSpark.\nServerless (AWS Lambda, Google Functions).\n\nAlso, Docker containers are stateless: any changes done inside a container will NOT be saved when the container is killed and started again. This is an advantage because it allows us to restore any container to its initial state in a reproducible manner, but you will have to store data elsewhere if you need to do so; a common way to do so is with volumes.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  }
]