[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manuel Andersen",
    "section": "",
    "text": "I’m an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\nI live in Chile, which is in South America, so some parts of this blog may be in English and others in Spanish."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Manuel Andersen",
    "section": "",
    "text": "I’m an Industrial Engineer and Master in Operations Research student at Universidad Adolfo Ibañez.\nI live in Chile, which is in South America, so some parts of this blog may be in English and others in Spanish."
  },
  {
    "objectID": "projects/HP/index.html",
    "href": "projects/HP/index.html",
    "title": "A better Harry Potter Book",
    "section": "",
    "text": "The Time In Between: This is a look at the developing relationship between Hermione and Ron set during the Deathly Hallows.\nHermione Granger Series: This fan rewrote the first four books from Hermione’s point of view only. The links to each storie:\n\nHermione Granger and the Philosopher’s Stone\nHermione Granger and the Chamber of Secrets\nHermione Granger and the Prisoner of Azkaban\nHermione Granger and the Goblet of Fire"
  },
  {
    "objectID": "projects/HP/index.html#some-links-to-fan-made-stories",
    "href": "projects/HP/index.html#some-links-to-fan-made-stories",
    "title": "A better Harry Potter Book",
    "section": "",
    "text": "The Time In Between: This is a look at the developing relationship between Hermione and Ron set during the Deathly Hallows.\nHermione Granger Series: This fan rewrote the first four books from Hermione’s point of view only. The links to each storie:\n\nHermione Granger and the Philosopher’s Stone\nHermione Granger and the Chamber of Secrets\nHermione Granger and the Prisoner of Azkaban\nHermione Granger and the Goblet of Fire"
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#terminology",
    "title": "Docker",
    "section": "Terminology",
    "text": "Terminology\n\nContainer: environment that runs an applications that is not dependent on the OS. Kind of like a lightweight VM. Containers are stateless; if you need to update the components inside, create another container instead.\nImage: template to create a container. Its components are defined by a Dockerfile.\nVolume: storage area detached from the container for maintaining state.\nForeground/interactive vs background/detached: a detached container runs in the background whereas an interactive container will usually have a terminal of some sort for interacting with.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#useful-links",
    "title": "Docker",
    "section": "Useful links",
    "text": "Useful links\n\nCheatsheet",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "href": "notes/Data Engineering/week-01/01-intro-to-docker.html#what-is-docker",
    "title": "Docker",
    "section": "What is Docker?",
    "text": "What is Docker?\nDocker is a containerization software that allows us to isolate software in a similar way to virtual machines but in a much leaner way.\nDocker delivers software in packages called containers. These containers are isolated from one another. If we run a data pipeline inside a container it is virtually isolated from the rest of the things running on the computer.\nA Docker image is a snapshot of a container that we can define to run our software, or in this case our data pipelines. By exporting our Docker images to Cloud providers such as Amazon Web Services or Google Cloud Platform we can run our containers there.\n\n\n\nscreenshot\n\n\nWe can run the container we have created through the docker image where we have specified and configured the environment beyond the host computer and essentially everywhere like - Google Cloud (Kubernetes), AWS Batch etc.\nDocker Image ensures reproducibility regardless of the machine as the images are identical. We can specify OS, programming languages, packages, database type, tools etc. This solves the problem of “Works on my computer but NoT iN yOuRs.”\nSo, the advantages of working with Docker?\n\nLocal Experiments: Helps to run things locally like your database also helps with testing like integration testing.\nIntegration Tests (CI/CD).\nReproducibility: Docker makes things run everywhere regardless of what you are using.\nRunning Pipelines on the Cloud (AWS Batch, Kubernetes Jobs).\nSpark.\nServerless (AWS Lambda, Google Functions).\n\nAlso, Docker containers are stateless: any changes done inside a container will NOT be saved when the container is killed and started again. This is an advantage because it allows us to restore any container to its initial state in a reproducible manner, but you will have to store data elsewhere if you need to do so; a common way to do so is with volumes.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Docker"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html",
    "title": "Postgres",
    "section": "",
    "text": "Postgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema: A database schema is considered the “blueprint” of a database which describes how the data may relate to other tables or other data models.\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.\n\n\n\n\n\npgcli official page.\n\n\n\n\nPostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\nIMPORTANT: These values are only meant for testing. Please change them for any serious project.\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\n\n\nscreenshot\n\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\n\n\nscreenshot\n\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working.\n\n\n\nIn your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\n\n\nscreenshot\n\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\n\n\nscreenshot\n\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#terminology",
    "title": "Postgres",
    "section": "",
    "text": "Postgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\npgcli: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.\nschema: A database schema is considered the “blueprint” of a database which describes how the data may relate to other tables or other data models.\nSQLAlchemy: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#useful-links",
    "title": "Postgres",
    "section": "",
    "text": "pgcli official page.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#what-is-postgressql",
    "title": "Postgres",
    "section": "",
    "text": "PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nIn later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\nFor running postgres we will use the oficial docker image of postgres, so we will use:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    postgres:13  \nhere:\n\npostgres:13 means that we are running the 13 version of the postgres image.\n\nwe also need to configure postgres, so we need some environment variables that we declare by using -e :\n\nPOSTGRES_USER is the username for logging into the database. We chose root.\nPOSTGRES_PASSWORD is the password for the database. We chose root.\n\nIMPORTANT: These values are only meant for testing. Please change them for any serious project.\n\nPOSTGRES_DB is the name that we will give the database. We chose ny_taxi.\n\nwe also need a volume directory for storing the data, declare by -v. The colon : separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n\n$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data\n\nPath names must be absolute. If you’re in a UNIX-like system, you can use pwd to print you local folder as a shortcut; this example should work with both bash and zsh shells, but fish will require you to remove the $.\nThis command will only work if you run it from a directory which contains the ny_taxi_postgres_data subdirectory you created above.\n\n\nThe -p is for port mapping. We map the default Postgres port to the same port in the host.\n\n5432:5432\n\nSo, remember that the container doesnt remember anything you do once you restart it, thats why we do what is call a mounting by telling the container we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call ny_taxi_postgres_data.\nSo, if everything is running smoothly we can try to acces the database, for this we will use something called pgcli, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:\npgcli \\\n    -h  localhost \\\n    -p 5432 \\\n    -u root \\\n    -d ny_taxi\nwhere:\n\nh is the host. Since we’re running locally we can use localhost.\n-p declares the port.\n-u is the username.\n-d is the database name.\n\nIf everything goes OK we will be in the pgcli interactive shell, where we can type \\dt to see a list of all the tables available in our database, wich will look something like this:\n\n\n\nscreenshot\n\n\nwhere we dont have anything because we havent create anything yet. We can also run a SQL type of command like select 1:\n\n\n\nscreenshot\n\n\nwich gives us nothing, but at least it assure us that we have a connection to the database and it is working.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "href": "notes/Data Engineering/week-01/02-ingesting-data.html#ingesting-data-to-postgres-with-python",
    "title": "Postgres",
    "section": "",
    "text": "In your terminal run:\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\nfor dowloading the parquet file.\nFor more convenient way, we will use a jupyter notebook file called upload_data.ipynb, we can look at the head of the data by doing:\nimport pandas as pd\n\nurl = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\ndf = pd.read_parquet(url, engine='pyarrow')\n\ndf.head()\nNow that we have the data we can put it into postgres. For this we need to generate what is called a schema. A schema is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called io that can output the SQL DDL (Data definition language) instructions necessary to create the schema.\n# We need to provide a name for the table; we will use 'yellow_taxi_data'\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data'))\nWhich outputs something like this:\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" INTEGER,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" INTEGER,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL,\n  \"airport_fee\" REAL\n)\nThe schema generated is not optimized as it “mistypes” columns, for example total_amount is declared as type REAL, but for now its OK.\nThis only outputs the instructions, it hasn’t actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use sqlalchemy for this.\nfrom sqlalchemy import create_engine\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nAn engine specifies the database details in a URI. The structure of the URI is:\ndatabase://user:password@host:port/database_name.\nso now we can connect the gine by doing:\n# run this cell when the Postgres Docker container is running\nengine.connect()\nand with this connection ready, we can use our engine to get the specific schema output for Postgres:\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))\nwich will give us something like this:\nCREATE TABLE yellow_taxi_data (\n    \"VendorID\" BIGINT, \n    tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n    tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n    passenger_count FLOAT(53), \n    trip_distance FLOAT(53), \n    \"RatecodeID\" FLOAT(53), \n    store_and_fwd_flag TEXT, \n    \"PULocationID\" BIGINT, \n    \"DOLocationID\" BIGINT, \n    payment_type BIGINT, \n    fare_amount FLOAT(53), \n    extra FLOAT(53), \n    mta_tax FLOAT(53), \n    tip_amount FLOAT(53), \n    tolls_amount FLOAT(53), \n    improvement_surcharge FLOAT(53), \n    total_amount FLOAT(53), \n    congestion_surcharge FLOAT(53), \n    airport_fee FLOAT(53)\n)\nSo, now we can start filling our database, but let’s notice that this data file is about 1369769 rows, so it’s quite big and is not wise (we may run into problems) to process all at once. So, we will use chunks of data in form of batches and iteratively send this batches to the database.\nFor this, we are using the pyarrow.parquet librarie to iterate over the data. We are going to use the parquet_file.iter_batches() function to create batches of 100,000, convert them into pandas and then load it into the postgres database.\nFirst, we want to create the table in postgres, for this we use:\n# Creating just the table in postgres\ndf.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')\nand if we go to our terminal were we have postgres and we do \\dt we will see something like this:\n\n\n\nscreenshot\n\n\nbut of course we dont have any data in there yet, because we havent send any of it:\n\n\n\nscreenshot\n\n\nNow for filling the databse we do:\n# Insert values into the table \nt_start = time()\ncount = 0\nfor batch in file.iter_batches(batch_size=100000):\n    count+=1\n    batch_df = batch.to_pandas()\n    print(f'inserting batch {count}...')\n    b_start = time()\n    \n    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')\n    b_end = time()\n    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\\n')\n    \nt_end = time()   \nprint(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    \nand if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "Postgres"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires.\n\n\n\n\n\npgAdmin official page.\n\n\n\n\nWe can start looking at our database in the postgres CLI by doing\nSELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount) \nFROM yellow_taxi_data;\nwich will give us something like this:\n\n\n\nscreenshot\n\n\nBut interacting this way with the database is not great. For a more convenient way we will use pgAdmin which is a web based GUI tool.\nInstead of installing it, we will use dokcer to install a container with pgAdmin, tu run the container we will use:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\nremembre, we use -e to declare environment variables, -p to do port mapping, so here:\n\nPGADMIN_DEFAULT_EMAIL is the default username we will use to login in pgAdmin.\nPGADMIN_DEFAULT_PASSWORD is the default password.\n8080:80 8080 is our local machine port and we will send request to the port 80 on the container.\ndpage/pgadmin4 is the pgAdmin image.\n\nwe the run this, and in our browser we go to localhost:8080 and we will see something like this:\n\n\n\nscreenshot\n\n\nhere we use the credentials we give as environment variables. And we will get something like:\n\n\n\nscreenshot\n\n\nWe now need to create a server, for wich we do:\n\n\n\nscreenshot\n\n\nWe then put a name to our server:\n\n\n\nscreenshot\n\n\nand then go to the Connection tab and add the Host name to localhost and use root as the username and password like we did with our engine to load the database, BUT we will encounter a problem:\n\n\n\nscreenshot\n\n\nWhy is that? It’s because we are running the pgAdmin inside a container, different to the container where we are running postgres. So, when we put localhost in the pgAdmin Host name, pgAdmin goes and look for localhost and it will be the container in wich we are running pgAdmin. We need to conect them, to do this we do Docker Network.\nFirst, let’s stop both containers. Then we need to create the docker network by doing:\ndocker  network create pg-network\nhere:\n\npg-network is the name of the network we want to create.\n\nNow, we need to initiate the postgres container in this way:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    --network=pg-network \\\n    --name pg-database \\\n    postgres:13  \nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-database is the name wich pgAdmin will use to discover the database.\n\nAnd now we need to initiate the pgAdmin container with:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  --name pg-admin \\\n  dpage/pgadmin4\nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-admin is the name of this container.\n\nNotes:\n\nIMPORTANT: these are example values for testing and should never be used on production. Change them accordingly when needed.\npgAdmin is a web app and its default port is 80; we map it to 8080 in our localhost to avoid any possible conflicts.\nJust like with the Postgres container, we specify a network and a name. However, the name in this example isn’t really necessary because there won’t be any containers trying to access this particular container.\n\nWe now run this and go to our webrowser and go to localhost:8080, and now we create a server, for wich we do:\n\n\n\nscreenshot\n\n\nWe then put a name to our server:\n\n\n\nscreenshot\n\n\nand then go to the Connection tab and add the Host name to pg-database and use root as the username and password like we did with our engine to load the database like this:\n\n\n\nscreenshot\n\n\nwe save, and we will have our server! Now, lets say we want to look at the first 100 rows we do:\n(Right Side Menu) &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; public &gt; Tables &gt; yellow_taxi_data &gt; (left_click) &gt; View/Edit Data &gt; First 100 Rows, and we will have something like this:\n\n\n\nscreenshot",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "pgAdmin"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#terminology",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#terminology",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin: PGAdmin is a web-based Graphical User Interface (GUI) management application used to communicate with Postgres and derivative relational databases on both local and remote servers.\nPostgres: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\nDocker networking: Docker networking enables a user to link a Docker container to as many networks as the user requires.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "pgAdmin"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#useful-links",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#useful-links",
    "title": "pgAdmin",
    "section": "",
    "text": "pgAdmin official page.",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "pgAdmin"
    ]
  },
  {
    "objectID": "notes/Data Engineering/week-01/03-pgAdmin.html#what-is-pgadmin",
    "href": "notes/Data Engineering/week-01/03-pgAdmin.html#what-is-pgadmin",
    "title": "pgAdmin",
    "section": "",
    "text": "We can start looking at our database in the postgres CLI by doing\nSELECT max(tpep_pickup_datetime), min(tpep_pickup_datetime), max(total_amount) \nFROM yellow_taxi_data;\nwich will give us something like this:\n\n\n\nscreenshot\n\n\nBut interacting this way with the database is not great. For a more convenient way we will use pgAdmin which is a web based GUI tool.\nInstead of installing it, we will use dokcer to install a container with pgAdmin, tu run the container we will use:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\nremembre, we use -e to declare environment variables, -p to do port mapping, so here:\n\nPGADMIN_DEFAULT_EMAIL is the default username we will use to login in pgAdmin.\nPGADMIN_DEFAULT_PASSWORD is the default password.\n8080:80 8080 is our local machine port and we will send request to the port 80 on the container.\ndpage/pgadmin4 is the pgAdmin image.\n\nwe the run this, and in our browser we go to localhost:8080 and we will see something like this:\n\n\n\nscreenshot\n\n\nhere we use the credentials we give as environment variables. And we will get something like:\n\n\n\nscreenshot\n\n\nWe now need to create a server, for wich we do:\n\n\n\nscreenshot\n\n\nWe then put a name to our server:\n\n\n\nscreenshot\n\n\nand then go to the Connection tab and add the Host name to localhost and use root as the username and password like we did with our engine to load the database, BUT we will encounter a problem:\n\n\n\nscreenshot\n\n\nWhy is that? It’s because we are running the pgAdmin inside a container, different to the container where we are running postgres. So, when we put localhost in the pgAdmin Host name, pgAdmin goes and look for localhost and it will be the container in wich we are running pgAdmin. We need to conect them, to do this we do Docker Network.\nFirst, let’s stop both containers. Then we need to create the docker network by doing:\ndocker  network create pg-network\nhere:\n\npg-network is the name of the network we want to create.\n\nNow, we need to initiate the postgres container in this way:\ndocker run -it \\\n    -e POSTGRES_USER=\"root\" \\\n    -e POSTGRES_PASSWORD=\"root\" \\\n    -e POSTGRES_DB=\"ny_taxi\" \\\n    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n    -p 5432:5432 \\\n    --network=pg-network \\\n    --name pg-database \\\n    postgres:13  \nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-database is the name wich pgAdmin will use to discover the database.\n\nAnd now we need to initiate the pgAdmin container with:\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  --network=pg-network \\\n  --name pg-admin \\\n  dpage/pgadmin4\nhere:\n\n--networking=pg-network is where we specify the name of the network.\n--name pg-admin is the name of this container.\n\nNotes:\n\nIMPORTANT: these are example values for testing and should never be used on production. Change them accordingly when needed.\npgAdmin is a web app and its default port is 80; we map it to 8080 in our localhost to avoid any possible conflicts.\nJust like with the Postgres container, we specify a network and a name. However, the name in this example isn’t really necessary because there won’t be any containers trying to access this particular container.\n\nWe now run this and go to our webrowser and go to localhost:8080, and now we create a server, for wich we do:\n\n\n\nscreenshot\n\n\nWe then put a name to our server:\n\n\n\nscreenshot\n\n\nand then go to the Connection tab and add the Host name to pg-database and use root as the username and password like we did with our engine to load the database like this:\n\n\n\nscreenshot\n\n\nwe save, and we will have our server! Now, lets say we want to look at the first 100 rows we do:\n(Right Side Menu) &gt; Docker localhost &gt; Databases &gt; ny_taxi &gt; Schemas &gt; public &gt; Tables &gt; yellow_taxi_data &gt; (left_click) &gt; View/Edit Data &gt; First 100 Rows, and we will have something like this:\n\n\n\nscreenshot",
    "crumbs": [
      "Data engineering",
      "Week 1",
      "pgAdmin"
    ]
  }
]