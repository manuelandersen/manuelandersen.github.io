---
title: Postgres 
date: last-modified
description: What is Postgres?
---

# Ingesting NY Taxi Data to Postgres

## Terminology

- `Postgres`: is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.
- `pgcli`: Pgcli is a command line interface for Postgres with auto-completion and syntax highlighting.
- `schema`: A database schema is considered the “blueprint” of a database which describes how the data may relate to other tables or other data models.
- `SQLAlchemy`: SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL.

## Useful links

- pgcli [official page](https://www.pgcli.com/).

## What is PostgresSQL?

**PostgreSQL** is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.

In later parts of the course we will use **Airflow**, which uses **PostgreSQL** internally. For simpler tests we can use **PostgreSQL** (or just **Postgres**) directly.

For running postgres we will use the oficial docker image of postgres, so we will use:


```bash
docker run -it \
    -e POSTGRES_USER="root" \
	-e POSTGRES_PASSWORD="root" \
	-e POSTGRES_DB="ny_taxi" \
    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \
	-p 5432:5432 \
    postgres:13  
```

here:

- `postgres:13` means that we are running the 13 version of the postgres image.

we also need to configure postgres, so we need some environment variables that we declare by using `-e` :

- `POSTGRES_USER`  is the username for logging into the database. We chose `root`.
- `POSTGRES_PASSWORD` is the password for the database. We chose `root`.
    - **IMPORTANT: These values are only meant for testing. Please change them for any serious project.**
- `POSTGRES_DB`  is the name that we will give the database. We chose `ny_taxi`.

we also need a `volume` directory for storing the data, declare by `-v`. The colon `:` separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).

- `$(pwd)/ny_taxi_postgres_data:var/lib/postgresql/data`  
    - Path names must be absolute. If you're in a UNIX-like system, you can use `pwd` to print you local folder as a shortcut; this example should work with both `bash` and `zsh` shells, but `fish` will require you to remove the `$`.
    - This command will only work if you run it from a directory which contains the `ny_taxi_postgres_data` subdirectory you created above.

The `-p` is for port mapping. We map the default Postgres port to the same port in the host.

- `5432:5432`

So, remember that the `container` doesnt remember anything you do once you restart it, thats why we do what is call a `mounting` by telling the `container` we want to map the folder with the data in our local machine to the folder in the container. So you need to create a folder in your project directory call `ny_taxi_postgres_data`.

So, if everything is running smoothly we can try to acces the database, for this we will use something called `pgcli`, wich stands for postgres cli, which enables us to interact with potsgresql database from the terminal and through Python script. So we can type:

```bash
pgcli \
	-h  localhost \
    -p 5432 \
	-u root \
	-d ny_taxi
```
where:

- `h` is the host. Since we're running locally we can use `localhost`.
- `-p` declares the port.
- `-u` is the username.
- `-d` is the database name.

If everything goes OK we will be in the pgcli interactive shell, where we can type `\dt` to see a list of all the tables available in our database, wich will look something like this:

![screenshot](../week-01/imgs/pgcline-dt.png)

where we dont have anything because we havent create anything yet. We can also run a `SQL` type of command like `select 1`:

![screenshot](../week-01/imgs/pgcline-1.png)

wich gives us nothing, but at least it assure us that we have a connection to the database and it is working.

## Ingesting data to Postgres with Python

In your terminal run:

```bash
wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet
```
for dowloading the parquet file. 

For more convenient way, we will use a `jupyter notebook` file called `upload_data.ipynb`, we can look at the head of the data by doing:

```python
import pandas as pd

url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet"

df = pd.read_parquet(url, engine='pyarrow')

df.head()
```

Now that we have the data we can put it into postgres. For this we need to generate what is called a `schema`. A `schema` is the structure of the database; is an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called an instruction to create a table that specifies what kind of columns you want to have there and their types. For this, pandas as a module called `io` that can output the SQL **DDL** (Data definition language) instructions necessary to create the schema.

```python
# We need to provide a name for the table; we will use 'yellow_taxi_data'
print(pd.io.sql.get_schema(df, name='yellow_taxi_data'))
```

Which outputs something like this:

```SQL
CREATE TABLE "yellow_taxi_data" (
"VendorID" INTEGER,
  "tpep_pickup_datetime" TIMESTAMP,
  "tpep_dropoff_datetime" TIMESTAMP,
  "passenger_count" REAL,
  "trip_distance" REAL,
  "RatecodeID" REAL,
  "store_and_fwd_flag" TEXT,
  "PULocationID" INTEGER,
  "DOLocationID" INTEGER,
  "payment_type" INTEGER,
  "fare_amount" REAL,
  "extra" REAL,
  "mta_tax" REAL,
  "tip_amount" REAL,
  "tolls_amount" REAL,
  "improvement_surcharge" REAL,
  "total_amount" REAL,
  "congestion_surcharge" REAL,
  "airport_fee" REAL
)
```
The schema generated is not optimized as it "mistypes" columns, for example `total_amount` is declared as type `REAL`, but for now its OK.

This only outputs the instructions, it hasn't actually created the table in the database yet. Even though we have the DDL instructions, we still need specific instructions for Postgres to connect to it and create the table. We will use **sqlalchemy** for this.

```python
from sqlalchemy import create_engine

engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')
```

An **engine** specifies the database details in a URI. The structure of the URI is:

`database://user:password@host:port/database_name`.

so now we can connect the gine by doing:

```python
# run this cell when the Postgres Docker container is running
engine.connect()
```

and with this connection ready, we can use our engine to get the specific schema output for Postgres:

```python
print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))
```
wich will give us something like this:

```SQL
CREATE TABLE yellow_taxi_data (
	"VendorID" BIGINT, 
	tpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, 
	tpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, 
	passenger_count FLOAT(53), 
	trip_distance FLOAT(53), 
	"RatecodeID" FLOAT(53), 
	store_and_fwd_flag TEXT, 
	"PULocationID" BIGINT, 
	"DOLocationID" BIGINT, 
	payment_type BIGINT, 
	fare_amount FLOAT(53), 
	extra FLOAT(53), 
	mta_tax FLOAT(53), 
	tip_amount FLOAT(53), 
	tolls_amount FLOAT(53), 
	improvement_surcharge FLOAT(53), 
	total_amount FLOAT(53), 
	congestion_surcharge FLOAT(53), 
	airport_fee FLOAT(53)
)
```

So, now we can start filling our database, but let's notice that this data file is about 1369769 rows, so it's quite big and is not wise (we may run into problems) to process all at once. So, we will use **chunks** of data in form of **batches** and iteratively send this **batches** to the database.

For this, we are using the `pyarrow.parquet` librarie to **iterate** over the data.  We are going to use the `parquet_file.iter_batches()` function to create batches of 100,000, convert them into pandas and then load it into the postgres database.

First, we want to create the table in postgres, for this we use:

```python
# Creating just the table in postgres
df.head(0).to_sql(name='yellow_taxi_data',con=engine, if_exists='replace')
```

and if we go to our terminal were we have postgres and we do `\dt` we will see something like this:

![screenshot](../week-01/imgs/first-table.png)

but of course we dont have any data in there yet, because we havent send any of it:

![screenshot](../week-01/imgs/first-look-data.png)

Now for filling the databse we do:

```python
# Insert values into the table 
t_start = time()
count = 0
for batch in file.iter_batches(batch_size=100000):
    count+=1
    batch_df = batch.to_pandas()
    print(f'inserting batch {count}...')
    b_start = time()
    
    batch_df.to_sql(name='ny_taxi_data',con=engine, if_exists='append')
    b_end = time()
    print(f'inserted! time taken {b_end-b_start:10.3f} seconds.\n')
    
t_end = time()   
print(f'Completed! Total time taken was {t_end-t_start:10.3f} seconds for {count} batches.')    
```

and if we wait a little time (for me it took 86.964 seconds for 14 batches) we will have all of our data in the postgres database.

