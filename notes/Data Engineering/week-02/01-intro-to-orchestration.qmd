---
title: Orchestration 
date: last-modified
description: Intro to Orchestration
---

# Module 1: 📯 Intro to Orchestration

## What is Orchestration?

A large part of data engineering is extracting, transforming and loading data between sources. Orchestration is a process of dependency management, facilitated trough automation. The data orchestrator manages scheduling, triggering, monitoring, adnd even resource allocation for you data engineering workflows. Every workflow requires sequential steps:

- Steps == tasks
- Workflows == DAG's (directed acyclic graphs) or Pipeline
- Data orchestrator:

![](./imgs/orchestrator.png)

# Module 2: 🧙‍♂️ Intro to Mage

## What is Mage?

Open-source data pipeline tool for transforming and integrating data.
The modern replacement for Airflow.

It consist of:

- Projects: forms the basis for all the work you can do in Mage - you can think of it like a Github repo. It contains the code for all of your pipelines, blocks and other assets.
- Pipelines: is a workflow that executes some data operation - maybe extracting, transforming and loading data from an API.
- Blocks: is a file that can be executed independently or within a pipeline.

## Configure Mage

First, let's clone this [repo](https://github.com/mage-ai/mage-zoomcamp):

```bash
git clone https://github.com/mage-ai/mage-zoomcamp
```

you go to the repo with `cd` and then rename `dev.env` to `.env` — this will ensure the file is not committed to Git by accident, since it will contain credentials in the future.

Now we can build the container:

```bash
docker compose build
```
wich has `mageai` and `postgres`. Now we can run the container by doing:

```bash
docker compose up
```
and then in our browser we go to `localhost:6789` we will get something like this:

![](./imgs/mageai.png)

wich is the `mage instance`.

## A simple Pipeline

Go to the `Pipelines` tab, wich gives you an overview of all of the pipelines that we have. The `docker compose` will have created a `Project` in mage called `magic-zoomcamp`, wich will have an `example_pipeline`. Here's how that pipeline looks:

![](./imgs/mage-pipeline.png)

you can play with it, but we are gonna use this to make a more interesting pipeline wich will send the data to postgres.

# Module 3: 🐘 ETL: API to Postgres

## Configuring Postgres

In order to write things into postgres we need to first connect to postgres, for this we are gonna do it the simple way, with the mage GUI. Go to the `Files` tab and then open the `io_config.yaml` file, which is were we manage our conections. Here we have a `default` profile, but let's create a new one called `dev`. 

```yaml
dev:
  # PostgresSQL
  POSTGRES_CONNECT_TIMEOUT: 10
  POSTGRES_DBNAME: "{{ env_var('POSTGRES_DBNAME') }}"
  POSTGRES_SCHEMA: "{{ env_var('POSTGRES_SCHEMA') }}"
  POSTGRES_USER: "{{ env_var('POSTGRES_USER') }}"
  POSTGRES_PASSWORD: "{{ env_var('POSTGRES_PASSWORD') }}"
  POSTGRES_HOST: "{{ env_var('POSTGRES_HOST') }}"
  POSTGRES_PORT: "{{ env_var('POSTGRES_PORT') }}"
```
Note that we are pulling the environment variables trough `jinja templating`, wich is the way we pull variables in mage.

We can now test this by making a new pipeline called `test_config` and make a ` SQLData Loader` block inside this called `test_postgres`, where we can define a `PostgreSQL` connection and the `dev` profile. We can run `SQL` code here wich will be run in `Postgres`, and we will be sure that connection is made:

```SQL
SELECT 1;
```

![](./imgs/mage-postgres-conection-1.png)

## ETL: API to Postgres

So, its not going to be an actual API, just a compress csv, but the idea is pretty similar.  First, lets create a new `Standard (batch)` pipeline called `api_to_postgres` and we are gonna edit the pipeline and create a `Data loader` for Python, and use an `API` template, and lets called it `load_api_data`.

::: {.callout-tip}
## Best Practice

Always map data types, in order to manage memory consumption.
:::

We use this python script:

```python
import io
import pandas as pd
import requests
if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_data_from_api(*args, **kwargs):
    """
    Template for loading data from API
    """
    url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz'
    taxi_dtypes = {
        'VendorID': pd.Int64Dtype(),
        'passenger_count': pd.Int64Dtype(),
        'trip_distance': float,
        'RatecodeID': pd.Int64Dtype(),
        'store_and_fwd_flag': str,
        'PULocationID': pd.Int64Dtype(),
        'DOLocationID': pd.Int64Dtype(),
        'payment_type': pd.Int64Dtype(),
        'fare_amount': float,
        'extra': float,
        'mta_tax': float,
        'tip_amount': float,
        'tolls_amount': float,
        'improvement_surcharge': float,
        'total_amount': float,
        'congestion_surcharge': float 
    }

    parse_dates = ['tpep_pickup_datetime', 'tpep_dropoff_datetime']

    return pd.read_csv(url, sep=",", compression="gzip", dtype=taxi_dtypes, parse_dates=parse_dates)


@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined'

```
Then we are gona transform the data, for this we create a generic `Transformer` block called `transform_taxi_data`.

```python
if 'transformer' not in globals():
    from mage_ai.data_preparation.decorators import transformer
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test

import pandas as pd

@transformer
def transform(data, *args, **kwargs):
    # Specify your transformation logic here

    zero_passengers_df = data[data['passenger_count'].isin([0])]
    zero_passengers_count = zero_passengers_df['passenger_count'].count()
    non_zero_passengers_df = data[data['passenger_count'] > 0]
    print(f'Preprocessing: records with zero passengers: {zero_passengers_count}')

    return non_zero_passengers_df


@test
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output['passenger_count'].isin([0]).sum() == 0, 'There are rides with zero passengers'
```

Now, we are gonna export the data to postgres using the `Data exporter` block.

```python
from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.postgres import Postgres
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_postgres(df: DataFrame, **kwargs) -> None:
    """
    Template for exporting data to a PostgreSQL database.
    Specify your configuration settings in 'io_config.yaml'.

    Docs: https://docs.mage.ai/design/data-loading#postgresql
    """
    schema_name = 'ny_taxi'  # Specify the name of the schema to export data to
    table_name = 'yellow_cab_data'  # Specify the name of the table to export data to
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'dev'

    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:
        loader.export(
            df,
            schema_name,
            table_name,
            index=False,  # Specifies whether to include index in exported table
            if_exists='replace',  # Specify resolution policy if table name already exists
        )
```

And now we can test our load by doing a SQL `Data loader` block and do:

```SQL
SELECT * 
FROM 
    ny_taxi.yellow_cab_data 
LIMIT 
    10
```

and we should see our data.

# Module 4:  🤓 ETL: API to GCS

Ok, so we've written data locally to a database, but what about the cloud? In this tutorial, we'll walk through the process of using Mage to extract, transform, and load data from an API to Google Cloud Storage (GCS).

We'll cover both writing partitioned and unpartitioned data to GCS and discuss why you might want to do one over the other. Many data teams start with extracting data from a source and writing it to a data lake before loading it to a structured data source, like a database.

## Configuring GCP

First of all, got to Google Cloud Platform and create a `Bucket`, then go and create a Service Account to allow Mage to connect to GCP. You can copy the JSON keys to your working folder. 

Now, in Mage, we can go to the terminal and look for the `Path` of the JSON keys and paste them in the `GOOGLE_SERVICE_ACC_KEY_FILPATH` inside the `io_config.yaml` file. We can test the connection by running the `test_postgres` pipeline, with `BigQuery` connection and with `default` Profile.

:::{.callout-note}
Check the "Use Raw SQL" block, otherwise you will get a 404 error.
:::

You can then run the `example_pipeline` to download the `titanic_clean.csv` into your local directory. Load this data into your bucket, and then delete `test_postgres` and create a new data loader called `test_gcs`. If you ran this, you will see that you can acces the data trough BigQuery.

## API to GCS

Leta create a new pipeline, and a cool thing about Mage its that we can reutilize code simply by dragging and dropping in the pipeline. So, lets reutilize the `load_api_data` Data loader and the `transform_taxi_data` Transformer. Then we create a Data Exporter called `taxi_to_gcs_parquet`:

```python
from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.google_cloud_storage import GoogleCloudStorage
from pandas import DataFrame
from os import path

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:
    """
    Template for exporting data to a Google Cloud Storage bucket.
    Specify your configuration settings in 'io_config.yaml'.

    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage
    """
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    bucket_name = 'mage-zoomcamp-manuel-andersen'
    object_key = 'nyc_taxi_data.parquet'

    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(
        df,
        bucket_name,
        object_key,
    )
```

we the run this pipeline by clicking `Execute with all upstream blocks` and we should see our data exported in parquet file to our bucket.

The next natural question is: If we have a larger dataset, do we want to export it into a single parquet file? No, so we are gonna write this into a partitioned parquet file structure.

## Partitioned parquet exporter

Lets create a Data exporter called `taxi_to_gcs_partitioned_parquet` and lets connec it to the upstream Transformer block (we can just drag the connection with the Tree associated to the pipeline in Mage).

```python
import pyarrow as pa
import pyarrow.parquet as pq
import os

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = "/home/src/lucky-history-422623-a7-b0f9ecbd2a23.json"

bucket_name = 'mage-zoomcamp-manuel-andersen'
project_id = 'lucky-history-422623-a7'

table_name = "nyc_taxi_data"

root_path = f'{bucket_name}/{table_name}'

@data_exporter
def export_data(data, *args, **kwargs):
    data['tpep_pickup_date'] = data['tpep_pickup_datetime'].dt.date

    table = pa.Table.from_pandas(data)

    gcs = pa.fs.GcsFileSystem()

    pq.write_to_dataset(
        table,
        root_path=root_path,
        partition_cols=['tpep_pickup_date'],
        filesystem=gcs
    )
```

here we are partitioning by date, so if you run this and then look at your bucket you should see a folder called `nyc_taxi_data` wich inside has a bunch of other folders, each one of those representing a date, inside this ones are the partitioned data.

# Module 5: 🔍 ETL: GCS to BigQuery

# Module 6: 👨‍💻 Parameterized Execution

# Module 7: 🤖 Deployment
